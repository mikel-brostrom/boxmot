# Tracking Module Documentation

This directory contains the core logic for object tracking, data vectorization, graph database interaction, and multimodal inference.

## Files

### `track.py`

This script is the main entry point for running the object tracking pipeline. It utilizes a YOLO model for object detection and various tracking algorithms (e.g., DeepOCSORT, BoT-SORT) to maintain object identities across frames.

**Key Functionalities:**
- Initializes and configures the chosen YOLO detector and object tracker.
- Processes video or image sequences frame by frame.
- For each frame:
    - Performs object detection.
    - Updates tracker states with new detections.
    - Optionally saves annotated frames, crops of tracked objects, and tracking results to text files.
- If `--save-dataset` is enabled:
    - Embeds frame images and detected entity crops using a vision model (e.g., CLIP).
    - Stores these embeddings and associated metadata (bounding boxes, class names, tracker IDs) in a Qdrant vector database.
    - Computes and stores relationships between entities within a frame (e.g., spatial relationships like 'near', 'left_of').
    - Saves all per-frame data, including entity information, embeddings, and relationships, into a `dataset.json` file.
- If `--metrics` is enabled:
    - Calculates and saves performance metrics, including processing times and embedding statistics, to `metrics.json`.
- Supports clearing previous run data with `--clear-prev-runs`.

**Environment Variables Used (via `vectorize.py` indirectly for Qdrant/model config):**
- `EMBEDDING_MODEL_NAME`: Specifies the embedding model to use (e.g., "clip-ViT-B-32").

**Command-line Arguments:**
The script accepts numerous arguments to customize its behavior, including:
- `--yolo-model`: Path to the YOLO model weights.
- `--reid-model`: Path to the ReID model weights (for trackers that use it).
- `--tracking-method`: The tracking algorithm to use.
- `--source`: Input video file, directory, URL, or webcam ID.
- `--conf`: Confidence threshold for detections.
- `--classes`: Filter detections by specific class IDs.
- `--save-dataset`: Enable saving of frame and entity data, including embeddings.
- `--frame-similarity-threshold`: Cosine similarity threshold to avoid redundant frame embeddings.
- `--metrics`: Enable saving of performance metrics.
- `--clear-prev-runs`: Clear output directory of previous runs.
- ... and many others for controlling output, display, and NMS.

### `vectorize.py`

This module handles the creation and management of vector embeddings for images (frames and crops) using SentenceTransformer models (e.g., CLIP). It interacts with a Qdrant vector database to store and search these embeddings.

**Key Functionalities:**
- Initializes the specified embedding model (controlled by `EMBEDDING_MODEL_NAME`).
- Provides functions to:
    - `embed_image()`: Generate an embedding for a full image.
    - `embed_crop()`: Generate an embedding for an image crop (typically of a detected entity).
    - `search_entity()`: Search for similar entities in Qdrant based on an embedding.
    - `add_entity()`: Add a new entity embedding and its metadata (including a base64 encoded image of the crop) to the Qdrant `entities` collection.
    - `add_frame_embedding()`: Add a new frame embedding and its metadata (including an optional base64 encoded frame image) to the Qdrant `frames` collection.
    - `get_entity_metadata()`: Retrieve the payload/metadata for a given vector ID from the `entities` collection.
    - `calculate_cosine_similarity()`: Compute the cosine similarity between two embedding vectors.
- `reinitialize_collections()`: Deletes and recreates Qdrant collections (`entities`, `frames`) with the dimension specified by the chosen embedding model. This is typically called at the start of a `track.py` run if `--clear-prev-runs` is used or for a fresh setup.

**Environment Variables Used:**
- `EMBEDDING_MODEL_NAME`: Specifies the SentenceTransformer model to use for embeddings (e.g., "clip-ViT-B-32", "clip-ViT-L-14"). Defaults to "clip-ViT-B-32".
- `QDRANT_HOST` (implicit): The host for the Qdrant service (defaults to 'localhost' in code).
- `QDRANT_PORT` (implicit): The port for the Qdrant service (defaults to 6333 in code).

**Qdrant Collections:**
- `entities`: Stores embeddings of detected object crops. Each point includes the vector, a unique ID, and a payload containing metadata like `entity_id`, `class`, `class_name`, `bbox`, and the base64 encoded crop image.
- `frames`: Stores embeddings of video frames. Each point includes the vector, a unique ID, and a payload containing `frame_idx`, `timestamp`, and optionally the base64 encoded frame image.

### `graphify.py`

This script takes the `dataset.json` file generated by `track.py` (when `--save-dataset` is used) and populates a Neo4j graph database with the structured information.

**Key Functionalities:**
- Connects to a Neo4j instance.
- `clear_graph()`: Optionally clears all existing data from the graph before processing.
- `create_constraints()`: Creates unique constraints on `Frame` nodes (by `frame_idx`) and `Entity` nodes (by `entity_id`) for data integrity and performance.
- `graphify_dataset()`:
    - Iterates through each frame in `dataset.json`.
    - Creates/updates `Frame` nodes with properties like `frame_idx`, `timestamp`, `geo`, `latest_vector_id` (from Qdrant), and a generated `name`.
    - Creates `:NEXT` and `:PREV` relationships between sequential `Frame` nodes.
    - Creates/updates `Entity` nodes with properties like `entity_id` (tracker ID or logical ID), `class`, `class_name`, `latest_vector_id` (from Qdrant), and a generated `name`.
    - Creates `:DETECTED_IN` relationships from `Entity` nodes to `Frame` nodes, storing `confidence` and the entity's `vector_id` for that specific detection instance as relationship properties.
    - Creates inter-entity relationships (e.g., `(:Entity)-[:near]->(:Entity)`) based on the `relationships` field in `dataset.json`. Relationship types are sanitized for Neo4j compatibility.

**Environment Variables Used:**
- `NEO4J_URI`: The URI for the Neo4j database (e.g., "bolt://localhost:7687").
- `NEO4J_USER`: The username for Neo4j authentication.
- `NEO4J_PASSWORD`: The password for Neo4j authentication.

**Neo4j Graph Schema:**
- **Nodes:**
    - `Frame`: Represents a single frame in the video. Properties include `frame_idx`, `timestamp`, `geo`, `latest_vector_id`, `name`.
    - `Entity`: Represents a tracked object. Properties include `entity_id`, `class`, `class_name`, `latest_vector_id`, `name`.
- **Relationships:**
    - `NEXT`: Connects a `Frame` to the subsequent `Frame`.
    - `PREV`: Connects a `Frame` to the preceding `Frame`.
    - `DETECTED_IN`: Connects an `Entity` to a `Frame` where it was detected. Properties: `confidence`, `vector_id`.
    - Dynamic relationship types (e.g., `near`, `left_of`): Connect `Entity` nodes based on computed relationships.

### `infer.py`

This script provides an interactive command-line interface for multimodal inference. It allows users to ask natural language queries, which are then processed by retrieving relevant visual context (images of entities and frames from Qdrant) and graph context (entity relationships and appearances from Neo4j). This consolidated information, along with the query and retrieved images, is then passed to an OpenAI multimodal model (e.g., GPT-4 Vision) to generate an answer.

**Key Functionalities:**
- Initializes and connects to:
    - SentenceTransformer model (for embedding the user's text query).
    - Qdrant (to retrieve similar images).
    - Neo4j (to retrieve graph context for entities found in images).
    - OpenAI API.
- `interactive_loop()`:
    - Takes user text query as input.
    - Embeds the query using `embed_text()`.
    - `retrieve_similar_images()`:
        - Searches Qdrant `entities` and `frames` collections using the query embedding.
        - Implements a retrieval strategy: fetches more images than needed initially (`RETRIEVAL_MULTIPLIER`), filters by a `MIN_SIMILARITY_THRESHOLD`, and then selects the top `TARGET_ENTITY_TOP_K` entity images and `TARGET_FRAME_TOP_K` frame images.
        - Decodes base64 images from Qdrant payloads into PIL Images.
    - `retrieve_graph_context()`:
        - Takes entity IDs extracted from the retrieved Qdrant entity images.
        - Queries Neo4j to get information about these entities, their frame appearances, and their relationships with each other.
    - `build_prompt()`: Constructs a prompt for the OpenAI model, including:
        - The original user query.
        - The retrieved graph context summary (text).
        - The retrieved PIL images (converted to base64 data URIs).
    - Sends the prompt to the specified OpenAI chat model.
    - Prints the assistant's response.
    - Logs the entire interaction (query, retrieved context, prompt, response, saved images) to a JSON file in `runs/track/exp/infer/<timestamp>/`.

**Environment Variables Used:**
- `OPENAI_MODEL`: The OpenAI chat model to use (e.g., "gpt-4-vision-preview").
- `EMBEDDING_MODEL_NAME`: The SentenceTransformer model for text query embedding.
- `OPENAI_API_KEY`: Your OpenAI API key.
- `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD`: Neo4j connection details.
- `QDRANT_HOST`, `QDRANT_PORT`: Qdrant connection details (defaults to localhost:6333 if not specified by args).

### `relate.py`

A simple module responsible for computing basic relationships between detected entities within a single frame.

**Key Functionalities:**
- `compute_relationships(entities: List[Dict])`:
    - Takes a list of entity dictionaries (containing `bbox`, `id`, `class_name`, etc.).
    - Currently, it focuses on relationships where the *subject* is a 'person'.
    - Calculates spatial relationships:
        - `near`: Based on Euclidean distance between bounding box centers (threshold: < 100 pixels).
        - `left_of`, `right_of`, `above`, `below`: Based on the relative positions of bounding box centers.
    - Returns a list of relationship triplets: `(subject_id, predicate_string, object_id)`.

**Note:** The relationship generation is currently basic and primarily focused on 'person' subjects. It can be extended to include more complex relationships or consider other entity types as subjects.

### `.env.example`

This file serves as a template for the required environment variables. Users should copy this to a `.env` file and fill in their actual credentials and configurations.

```
OPENAI_MODEL=gpt-4-vision-preview
EMBEDDING_MODEL_NAME=clip-ViT-B-32
OPENAI_API_KEY=your_openai_api_key_here
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
QDRANT_HOST=localhost
QDRANT_PORT=6333
```

## Workflow Overview

1.  **Tracking & Data Generation (`track.py`):**
    *   Run `track.py` with a video source and the `--save-dataset` flag.
    *   This script performs object tracking.
    *   Frame images and entity crops are embedded using `vectorize.py` and stored in Qdrant.
    *   A `dataset.json` file is created in `runs/track/exp/dataset/`, containing structured data about frames, entities, their Qdrant vector IDs, and basic relationships computed by `relate.py`.

2.  **Graph Population (`graphify.py`):**
    *   Run `graphify.py` (optionally specifying the path to `dataset.json`).
    *   This script parses `dataset.json` and populates the Neo4j graph database with nodes for frames and entities, and relationships between them.

3.  **Interactive Inference (`infer.py`):**
    *   Run `infer.py`.
    *   Ask natural language questions.
    *   The script will:
        *   Embed your query.
        *   Retrieve relevant images from Qdrant.
        *   Fetch graph context for entities in those images from Neo4j.
        *   Send all information to an OpenAI model for a response.

## Setup & Dependencies

- Ensure Python environment is set up with all packages from `pyproject.toml` (or `requirements.txt` if available).
- **Qdrant:** Must be running. A `docker-compose.yml` is provided in the root directory for easy setup.
- **Neo4j:** Must be running and accessible. The `docker-compose.yml` also includes a Neo4j service.
- **OpenAI API Key:** Required for `infer.py`.
- Create a `.env` file in the `tracking` directory (or project root if `python-dotenv` is configured to search there) with the necessary environment variables (see `.env.example`).

## Running the System

1.  **Start Services:**
    ```bash
    docker-compose up -d qdrant neo4j
    ```
    Wait for services to be healthy. Neo4j might take a minute or two on the first run.

2.  **Run Tracking and Dataset Generation:**
    ```bash
    python tracking/track.py --source <your_video_source> --save-dataset --metrics --clear-prev-runs --name my_tracking_experiment
    ```
    Adjust arguments as needed.

3.  **Populate the Graph Database:**
    The `dataset.json` path will be something like `runs/track/my_tracking_experiment/dataset/dataset.json`.
    ```bash
    python tracking/graphify.py --dataset-path runs/track/my_tracking_experiment/dataset/dataset.json
    ```

4.  **Run Interactive Inference:**
    ```bash
    python tracking/infer.py
    ```
    Then, type your questions at the prompt.
    (Ensure your environment variables for OpenAI, Neo4j, and Qdrant are correctly set in your `.env` file or passed as arguments if applicable). 