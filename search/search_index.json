{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BoxMOT: Pluggable SOTA multi-object tracking modules for segmentation, object detection and pose estimation models","text":""},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li> <p>Pluggable Architecture   Easily swap in/out SOTA multi-object trackers.</p> </li> <li> <p>Universal Model Support   Integrate with any segmentation, object-detection and pose-estimation models that outputs bounding boxes</p> </li> <li> <p>Benchmark-Ready   Local evaluation pipelines for MOT17, MOT20, and DanceTrack ablation datasets with \"official\" ablation detectors</p> </li> <li> <p>Performance Modes</p> </li> <li>Motion-only: for lightweight, CPU-efficient, high-FPS performance </li> <li> <p>Motion + Appearance: Combines motion cues with appearance embeddings (CLIPReID, LightMBN, OSNet) to maximize identity consistency and accuracy at a higher computational cost</p> </li> <li> <p>Reusable Detections &amp; Embeddings   Save once, run evaluations with no redundant preprocessing lightning fast.</p> </li> </ul>"},{"location":"#benchmark-results-mot17-ablation-split","title":"\ud83d\udcca Benchmark Results (MOT17 ablation split)","text":"Tracker Status HOTA\u2191 MOTA\u2191 IDF1\u2191 FPS boosttrack \u2705 69.253 75.914 83.206 25 botsort \u2705 68.885 78.222 81.344 46 hybridsort \u2705 68.216 76.382 81.164 25 strongsort \u2705 68.05 76.185 80.763 17 deepocsort \u2705 67.796 75.868 80.514 12 bytetrack \u2705 67.68 78.039 79.157 1265 ocsort \u2705 66.441 74.548 77.899 1483 <p><sub> NOTES: Evaluation was conducted on the second half of the MOT17 training set, as the validation set is not publicly available and the ablation detector was trained on the first half. We employed pre-generated detections and embeddings. Each tracker was configured using the default parameters from their official repositories. </sub></p>"},{"location":"#installation","title":"\ud83d\udd27 Installation","text":"<p>Install the <code>boxmot</code> package, including all requirements, in a Python&gt;=3.9 environment:</p> <pre><code>pip install boxmot\n</code></pre> <p>If you want to contribute to this package check how to contribute here</p>"},{"location":"#cli","title":"\ud83d\udcbb CLI","text":"<p>BoxMOT provides a unified CLI <code>boxmot</code> with the following subcommands:</p> <pre><code>Usage: boxmot COMMAND [ARGS]...\n\nCommands:\n  eval      Evaluate tracking performance\n  export    Export ReID models\n  generate  Generate detections and embeddings\n  track     Run tracking only\n  tune      Tune models via evolutionary algorithms\n</code></pre>"},{"location":"#python","title":"\ud83d\udc0d PYTHON","text":"<p>Seamlessly integrate BoxMOT directly into your Python MOT applications with your custom model.</p> <pre><code>import cv2\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot import BoostTrack\nfrom torchvision.models.detection import (\n    fasterrcnn_resnet50_fpn_v2,\n    FasterRCNN_ResNet50_FPN_V2_Weights as Weights\n)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load detector with pretrained weights and preprocessing transforms\nweights = Weights.DEFAULT\ndetector = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\ndetector.to(device).eval()\ntransform = weights.transforms()\n\n# Initialize tracker\ntracker = BoostTrack(reid_weights=Path('osnet_x0_25_msmt17.pt'), device=device, half=False)\n\n# Start video capture\ncap = cv2.VideoCapture(0)\n\nwith torch.inference_mode():\n    while True:\n        success, frame = cap.read()\n        if not success:\n            break\n\n        # Convert frame to RGB and prepare for detector\n        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        tensor = torch.from_numpy(rgb).permute(2, 0, 1).to(torch.uint8)\n        input_tensor = transform(tensor).to(device)\n\n        # Run detection\n        output = detector([input_tensor])[0]\n        scores = output['scores'].cpu().numpy()\n        keep = scores &gt;= 0.5\n\n        # Prepare detections for tracking\n        boxes = output['boxes'][keep].cpu().numpy()\n        labels = output['labels'][keep].cpu().numpy()\n        filtered_scores = scores[keep]\n        detections = np.concatenate([boxes, filtered_scores[:, None], labels[:, None]], axis=1)\n\n        # Update tracker and draw results\n        #   INPUT:  M X (x, y, x, y, conf, cls)\n        #   OUTPUT: M X (x, y, x, y, id, conf, cls, ind)\n        res = tracker.update(detections, frame)\n        tracker.plot_results(frame, show_trajectories=True)\n\n        # Show output\n        cv2.imshow('BoXMOT + Torchvision', frame)\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n\n# Clean up\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"#code-examples-tutorials","title":"\ud83d\udcdd Code Examples &amp; Tutorials","text":"Tracking <pre><code>$ boxmot track --yolo-model rf-detr-base.pt    # bboxes only\n  boxmot track --yolo-model yolox_s.pt         # bboxes only\n  boxmot track --yolo-model yolo12n.pt         # bboxes only\n  boxmot track --yolo-model yolo11n.pt         # bboxes only\n  boxmot track --yolo-model yolov10n.pt        # bboxes only\n  boxmot track --yolo-model yolov9c.pt         # bboxes only\n  boxmot track --yolo-model yolov8n.pt         # bboxes only\n                            yolov8n-seg.pt     # bboxes + segmentation masks\n                            yolov8n-pose.pt    # bboxes + pose estimation\n</code></pre> Tracking methods <pre><code>$ boxmot track --tracking-method deepocsort\n                                 strongsort\n                                 ocsort\n                                 bytetrack\n                                 botsort\n                                 boosttrack\n</code></pre> Tracking sources  Tracking can be run on most video formats  <pre><code>$ boxmot track --source 0                               # webcam\n                        img.jpg                         # image\n                        vid.mp4                         # video\n                        path/                           # directory\n                        path/*.jpg                      # glob\n                        'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n                        'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n</code></pre> Select ReID model  Some tracking methods combine appearance description and motion in the process of tracking. For those which use appearance, you can choose a ReID model based on your needs from this [ReID model zoo](https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO). These model can be further optimized for you needs by the [reid_export.py](https://github.com/mikel-brostrom/boxmot/blob/master/boxmot/appearance/reid/export.py) script  <pre><code>$ boxmot track --source 0 --reid-model lmbn_n_cuhk03_d.pt               # lightweight\n                                       osnet_x0_25_market1501.pt\n                                       mobilenetv2_x1_4_msmt17.engine\n                                       resnet50_msmt17.onnx\n                                       osnet_x1_0_msmt17.pt\n                                       clip_market1501.pt               # heavy\n                                       clip_vehicleid.pt\n                                      ...\n</code></pre> Filter tracked classes  By default the tracker tracks all MS COCO classes.  If you want to track a subset of the classes that you model predicts, add their corresponding index after the classes flag,  <pre><code>boxmot track --source 0 --yolo-model yolov8s.pt --classes 16 17  # COCO yolov8 model. Track cats and dogs, only\n</code></pre>  [Here](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) is a list of all the possible objects that a Yolov8 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero   Evaluation  Evaluate a combination of detector, tracking method and ReID model on standard MOT dataset or you custom one by  <pre><code># reproduce MOT17 README results\n$ boxmot eval --yolo-model yolox_x_MOT17_ablation.pt --reid-model lmbn_n_duke.pt --tracking-method boosttrack --source MOT17-ablation --verbose \n# MOT20 results\n$ boxmot eval --yolo-model yolox_x_MOT20_ablation.pt --reid-model lmbn_n_duke.pt --tracking-method boosttrack --source MOT20-ablation --verbose \n# Dancetrack results\n$ boxmot eval --yolo-model yolox_x_dancetrack_ablation.pt --reid-model lmbn_n_duke.pt --tracking-method boosttrack --source dancetrack-ablation --verbose \n# metrics on custom dataset\n$ boxmot eval --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --tracking-method deepocsort  --source ./assets/MOT17-mini/train --verbose\n</code></pre>  add `--gsi` to your command for postprocessing the MOT results by gaussian smoothed interpolation. Detections and embeddings are stored for the selected YOLO and ReID model respectively. They can then be loaded into any tracking algorithm. Avoiding the overhead of repeatedly generating this data.  Evolution  We use a fast and elitist multiobjective genetic algorithm for tracker hyperparameter tuning. By default the objectives are: HOTA, MOTA, IDF1. Run it by  <pre><code># saves dets and embs under ./runs/dets_n_embs separately for each selected yolo and reid model\n$ boxmot generate --source ./assets/MOT17-mini/train --yolo-model yolov8n.pt yolov8s.pt --reid-model weights/osnet_x0_25_msmt17.pt\n# evolve parameters for specified tracking method using the selected detections and embeddings generated in the previous step\n$ boxmot tune --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --n-trials 9 --tracking-method botsort --source ./assets/MOT17-mini/train\n</code></pre>  The set of hyperparameters leading to the best HOTA result are written to the tracker's config file.   Export  We support ReID model export to ONNX, OpenVINO, TorchScript and TensorRT  <pre><code># export to ONNX\n$ boxmot export --weights weights/osnet_x0_25_msmt17.pt --include onnx --device cpu\n# export to OpenVINO\n$ boxmot export --weights weights/osnet_x0_25_msmt17.pt --include openvino --device cpu\n# export to TensorRT with dynamic input\n$ boxmot export --weights weights/osnet_x0_25_msmt17.pt --include engine --device 0 --dynamic\n</code></pre> Example Description Notebook Torchvision bounding box tracking with BoxMOT Torchvision pose tracking with BoxMOT Torchvision segmentation tracking with BoxMOT"},{"location":"#contributors","title":"Contributors","text":""},{"location":"#contact","title":"Contact","text":"<p>For BoxMOT bugs and feature requests please visit GitHub Issues. For business inquiries or professional support requests please send an email to: box-mot@outlook.com</p>"},{"location":"quickstart/","title":"Tracking","text":"<pre><code>$ boxmot track --yolo-model rf-detr-base.pt    # bboxes only\n  boxmot track --yolo-model yolox_s.pt         # bboxes only\n  boxmot track --yolo-model yolo12n.pt         # bboxes only\n  boxmot track --yolo-model yolo11n.pt         # bboxes only\n  boxmot track --yolo-model yolov10n.pt        # bboxes only\n  boxmot track --yolo-model yolov9c.pt         # bboxes only\n  boxmot track --yolo-model yolov8n.pt         # bboxes only\n                            yolov8n-seg.pt     # bboxes + segmentation masks\n                            yolov8n-pose.pt    # bboxes + pose estimation\n</code></pre>"},{"location":"quickstart/#tracking-methods","title":"Tracking methods","text":"<pre><code>$ boxmot track --tracking-method deepocsort\n                                 strongsort\n                                 ocsort\n                                 bytetrack\n                                 botsort\n                                 boosttrack\n</code></pre>"},{"location":"quickstart/#tracking-sources","title":"Tracking sources","text":"<pre><code>$ boxmot track --source 0                               # webcam\n                        img.jpg                         # image\n                        vid.mp4                         # video\n                        path/                           # directory\n                        path/*.jpg                      # glob\n                        'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n                        'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n</code></pre>"},{"location":"quickstart/#select-reid-model","title":"Select ReID model","text":"<pre><code>$ boxmot track --source 0 --reid-model lmbn_n_cuhk03_d.pt               # lightweight\n                                       osnet_x0_25_market1501.pt\n                                       mobilenetv2_x1_4_msmt17.engine\n                                       resnet50_msmt17.onnx\n                                       osnet_x1_0_msmt17.pt\n                                       clip_market1501.pt               # heavy\n                                       clip_vehicleid.pt\n                                      ...\n</code></pre>"},{"location":"quickstart/#filter-tracked-classes","title":"Filter tracked classes","text":"<pre><code>boxmot track --source 0 --yolo-model yolov8s.pt --classes 16 17  # COCO yolov8 model. Track cats and dogs, only\n</code></pre>"},{"location":"quickstart/#evaluation","title":"Evaluation","text":"<pre><code># reproduce MOT17 README results\n$ boxmot eval --yolo-model yolox_x_MOT17_ablation.pt --reid-model lmbn_n_duke.pt --tracking-method boosttrack --source MOT17-ablation --verbose \n# MOT20 results\n$ boxmot eval --yolo-model yolox_x_MOT20_ablation.pt --reid-model lmbn_n_duke.pt --tracking-method boosttrack --source MOT20-ablation --verbose \n# Dancetrack results\n$ boxmot eval --yolo-model yolox_x_dancetrack_ablation.pt --reid-model lmbn_n_duke.pt --tracking-method boosttrack --source dancetrack-ablation --verbose \n# metrics on custom dataset\n$ boxmot eval --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --tracking-method deepocsort  --source ./assets/MOT17-mini/train --verbose\n</code></pre>"},{"location":"quickstart/#evolution","title":"Evolution","text":"<pre><code># saves dets and embs under ./runs/dets_n_embs separately for each selected yolo and reid model\n$ boxmot generate --source ./assets/MOT17-mini/train --yolo-model yolov8n.pt yolov8s.pt --reid-model weights/osnet_x0_25_msmt17.pt\n# evolve parameters for specified tracking method using the selected detections and embeddings generated in the previous step\n$ boxmot tune --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --n-trials 9 --tracking-method botsort --source ./assets/MOT17-mini/train\n</code></pre>"},{"location":"quickstart/#export","title":"Export","text":"<pre><code># export to ONNX\n$ boxmot export --weights weights/osnet_x0_25_msmt17.pt --include onnx --device cpu\n# export to OpenVINO\n$ boxmot export --weights weights/osnet_x0_25_msmt17.pt --include openvino --device cpu\n# export to TensorRT with dynamic input\n$ boxmot export --weights weights/osnet_x0_25_msmt17.pt --include engine --device 0 --dynamic\n</code></pre>"},{"location":"modes/eval/","title":"eval","text":""},{"location":"modes/eval/#boxmot-eval","title":"boxmot eval","text":"<p>Evaluate tracking performance</p> <p>Usage:</p> <pre><code>boxmot eval [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--source</code> text file/dir/URL/glob, 0 for webcam <code>0</code> <code>--imgsz</code>, <code>--img-size</code> parse_tuple inference size w,h (e.g. 640,480 or 640x480) None <code>--fps</code> integer video frame-rate <code>30</code> <code>--conf</code> float min confidence threshold <code>0.01</code> <code>--iou</code> float IoU threshold for NMS <code>0.7</code> <code>--device</code> text cuda device(s), e.g. 0 or 0,1,2,3 or cpu `` <code>--project</code> Path save results to project/name <code>/home/runner/work/boxmot/boxmot/runs</code> <code>--name</code> text save results to project/name `` <code>--exist-ok</code> boolean existing project/name ok, do not increment <code>True</code> <code>--half</code> boolean use FP16 half-precision inference <code>False</code> <code>--vid-stride</code> integer video frame-rate stride <code>1</code> <code>--ci</code> boolean reuse existing runs in CI (no UI) <code>False</code> <code>--tracking-method</code> text deepocsort, botsort, strongsort, ... <code>deepocsort</code> <code>--verbose</code> boolean print detailed logs <code>False</code> <code>--agnostic-nms</code> boolean class-agnostic NMS <code>False</code> <code>--gsi</code> boolean apply Gaussian smoothing interpolation <code>False</code> <code>--show</code> boolean display tracking in a window <code>False</code> <code>--show-labels</code> / <code>--hide-labels</code> boolean show or hide detection labels <code>True</code> <code>--show-conf</code> / <code>--hide-conf</code> boolean show or hide detection confidences <code>True</code> <code>--show-trajectories</code> boolean overlay past trajectories <code>False</code> <code>--save-txt</code> boolean save results to a .txt file <code>False</code> <code>--save-crop</code> boolean save cropped detections <code>False</code> <code>--save</code> boolean save annotated video <code>False</code> <code>--line-width</code> integer bounding box line width <code>Sentinel.UNSET</code> <code>--per-class</code> boolean track each class separately <code>False</code> <code>--target-id</code> integer ID to highlight in green None <code>--yolo-model</code> Path one or more YOLO weights for detection <code>[PosixPath('/home/runner/work/boxmot/boxmot/boxmot/engine/weights/yolov8n.pt')]</code> <code>--reid-model</code> Path one or more ReID model weights <code>[PosixPath('/home/runner/work/boxmot/boxmot/boxmot/engine/weights/osnet_x0_25_msmt17.pt')]</code> <code>--classes</code> integer filter by class indices <code>[0]</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"modes/generate/","title":"generate","text":""},{"location":"modes/generate/#boxmot-generate","title":"boxmot generate","text":"<p>Generate detections and embeddings</p> <p>Usage:</p> <pre><code>boxmot generate [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--source</code> text file/dir/URL/glob, 0 for webcam <code>0</code> <code>--imgsz</code>, <code>--img-size</code> parse_tuple inference size w,h (e.g. 640,480 or 640x480) None <code>--fps</code> integer video frame-rate <code>30</code> <code>--conf</code> float min confidence threshold <code>0.01</code> <code>--iou</code> float IoU threshold for NMS <code>0.7</code> <code>--device</code> text cuda device(s), e.g. 0 or 0,1,2,3 or cpu `` <code>--project</code> Path save results to project/name <code>/home/runner/work/boxmot/boxmot/runs</code> <code>--name</code> text save results to project/name `` <code>--exist-ok</code> boolean existing project/name ok, do not increment <code>True</code> <code>--half</code> boolean use FP16 half-precision inference <code>False</code> <code>--vid-stride</code> integer video frame-rate stride <code>1</code> <code>--ci</code> boolean reuse existing runs in CI (no UI) <code>False</code> <code>--tracking-method</code> text deepocsort, botsort, strongsort, ... <code>deepocsort</code> <code>--verbose</code> boolean print detailed logs <code>False</code> <code>--agnostic-nms</code> boolean class-agnostic NMS <code>False</code> <code>--gsi</code> boolean apply Gaussian smoothing interpolation <code>False</code> <code>--show</code> boolean display tracking in a window <code>False</code> <code>--show-labels</code> / <code>--hide-labels</code> boolean show or hide detection labels <code>True</code> <code>--show-conf</code> / <code>--hide-conf</code> boolean show or hide detection confidences <code>True</code> <code>--show-trajectories</code> boolean overlay past trajectories <code>False</code> <code>--save-txt</code> boolean save results to a .txt file <code>False</code> <code>--save-crop</code> boolean save cropped detections <code>False</code> <code>--save</code> boolean save annotated video <code>False</code> <code>--line-width</code> integer bounding box line width <code>Sentinel.UNSET</code> <code>--per-class</code> boolean track each class separately <code>False</code> <code>--target-id</code> integer ID to highlight in green None <code>--yolo-model</code> Path one or more YOLO weights for detection <code>[PosixPath('/home/runner/work/boxmot/boxmot/boxmot/engine/weights/yolov8n.pt')]</code> <code>--reid-model</code> Path one or more ReID model weights <code>[PosixPath('/home/runner/work/boxmot/boxmot/boxmot/engine/weights/osnet_x0_25_msmt17.pt')]</code> <code>--classes</code> integer filter by class indices <code>[0]</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"modes/track/","title":"track","text":""},{"location":"modes/track/#boxmot-track","title":"boxmot track","text":"<p>Run tracking only</p> <p>Usage:</p> <pre><code>boxmot track [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--source</code> text file/dir/URL/glob, 0 for webcam <code>0</code> <code>--imgsz</code>, <code>--img-size</code> parse_tuple inference size w,h (e.g. 640,480 or 640x480) None <code>--fps</code> integer video frame-rate <code>30</code> <code>--conf</code> float min confidence threshold <code>0.01</code> <code>--iou</code> float IoU threshold for NMS <code>0.7</code> <code>--device</code> text cuda device(s), e.g. 0 or 0,1,2,3 or cpu `` <code>--project</code> Path save results to project/name <code>/home/runner/work/boxmot/boxmot/runs</code> <code>--name</code> text save results to project/name `` <code>--exist-ok</code> boolean existing project/name ok, do not increment <code>True</code> <code>--half</code> boolean use FP16 half-precision inference <code>False</code> <code>--vid-stride</code> integer video frame-rate stride <code>1</code> <code>--ci</code> boolean reuse existing runs in CI (no UI) <code>False</code> <code>--tracking-method</code> text deepocsort, botsort, strongsort, ... <code>deepocsort</code> <code>--verbose</code> boolean print detailed logs <code>False</code> <code>--agnostic-nms</code> boolean class-agnostic NMS <code>False</code> <code>--gsi</code> boolean apply Gaussian smoothing interpolation <code>False</code> <code>--show</code> boolean display tracking in a window <code>False</code> <code>--show-labels</code> / <code>--hide-labels</code> boolean show or hide detection labels <code>True</code> <code>--show-conf</code> / <code>--hide-conf</code> boolean show or hide detection confidences <code>True</code> <code>--show-trajectories</code> boolean overlay past trajectories <code>False</code> <code>--save-txt</code> boolean save results to a .txt file <code>False</code> <code>--save-crop</code> boolean save cropped detections <code>False</code> <code>--save</code> boolean save annotated video <code>False</code> <code>--line-width</code> integer bounding box line width <code>Sentinel.UNSET</code> <code>--per-class</code> boolean track each class separately <code>False</code> <code>--target-id</code> integer ID to highlight in green None <code>--yolo-model</code> Path path to YOLO weights for detection <code>/home/runner/work/boxmot/boxmot/boxmot/engine/weights/yolov8n.pt</code> <code>--reid-model</code> Path path to ReID model weights <code>/home/runner/work/boxmot/boxmot/boxmot/engine/weights/osnet_x0_25_msmt17.pt</code> <code>--classes</code> integer filter by class indices <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"modes/tune/","title":"tune","text":""},{"location":"modes/tune/#boxmot-tune","title":"boxmot tune","text":"<p>Tune models via evolutionary algorithms</p> <p>Usage:</p> <pre><code>boxmot tune [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--source</code> text file/dir/URL/glob, 0 for webcam <code>0</code> <code>--imgsz</code>, <code>--img-size</code> parse_tuple inference size w,h (e.g. 640,480 or 640x480) None <code>--fps</code> integer video frame-rate <code>30</code> <code>--conf</code> float min confidence threshold <code>0.01</code> <code>--iou</code> float IoU threshold for NMS <code>0.7</code> <code>--device</code> text cuda device(s), e.g. 0 or 0,1,2,3 or cpu `` <code>--project</code> Path save results to project/name <code>/home/runner/work/boxmot/boxmot/runs</code> <code>--name</code> text save results to project/name `` <code>--exist-ok</code> boolean existing project/name ok, do not increment <code>True</code> <code>--half</code> boolean use FP16 half-precision inference <code>False</code> <code>--vid-stride</code> integer video frame-rate stride <code>1</code> <code>--ci</code> boolean reuse existing runs in CI (no UI) <code>False</code> <code>--tracking-method</code> text deepocsort, botsort, strongsort, ... <code>deepocsort</code> <code>--verbose</code> boolean print detailed logs <code>False</code> <code>--agnostic-nms</code> boolean class-agnostic NMS <code>False</code> <code>--gsi</code> boolean apply Gaussian smoothing interpolation <code>False</code> <code>--show</code> boolean display tracking in a window <code>False</code> <code>--show-labels</code> / <code>--hide-labels</code> boolean show or hide detection labels <code>True</code> <code>--show-conf</code> / <code>--hide-conf</code> boolean show or hide detection confidences <code>True</code> <code>--show-trajectories</code> boolean overlay past trajectories <code>False</code> <code>--save-txt</code> boolean save results to a .txt file <code>False</code> <code>--save-crop</code> boolean save cropped detections <code>False</code> <code>--save</code> boolean save annotated video <code>False</code> <code>--line-width</code> integer bounding box line width <code>Sentinel.UNSET</code> <code>--per-class</code> boolean track each class separately <code>False</code> <code>--target-id</code> integer ID to highlight in green None <code>--n-trials</code> integer number of trials for evolutionary tuning <code>4</code> <code>--objectives</code> text objectives for tuning: HOTA, MOTA, IDF1 <code>['HOTA', 'MOTA', 'IDF1']</code> <code>--yolo-model</code> Path one or more YOLO weights for detection <code>[PosixPath('/home/runner/work/boxmot/boxmot/boxmot/engine/weights/yolov8n.pt')]</code> <code>--reid-model</code> Path one or more ReID model weights <code>[PosixPath('/home/runner/work/boxmot/boxmot/boxmot/engine/weights/osnet_x0_25_msmt17.pt')]</code> <code>--classes</code> integer filter by class indices <code>[0]</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"trackers/boosttrack/","title":"BoostTrack","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Initialize the BoostTrack tracker with various parameters.</p> <p>Parameters: - reid_weights (Path): Path to the re-identification model weights. - device (torch.device): Device to run the model on (e.g., 'cpu', 'cuda'). - half (bool): Whether to use half-precision (fp16) for faster inference. - det_thresh (float): Detection threshold for considering detections. - max_age (int): Maximum age (in frames) of a track before it is considered lost. - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5. - min_hits (int): Minimum number of detection hits before a track is considered confirmed. - iou_threshold (float): IOU threshold for determining match between detection and tracks. - per_class (bool): Enables class-separated tracking. - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True). - asso_func (str): Algorithm name used for data association between detections and tracks. - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.</p> <p>BoostTrack-specific parameters: - use_ecc (bool): Whether to use ECC for camera motion compensation. - min_box_area (int): Minimum box area for detections. - aspect_ratio_thresh (float): Aspect ratio threshold for detections. - cmc_method (str): Method for camera motion compensation. - lambda_iou (float): Weight for IoU-based association. - lambda_mhd (float): Weight for Mahalanobis distance-based association. - lambda_shape (float): Weight for shape-based association. - use_dlo_boost (bool): Whether to use DLO boost for confidence adjustment. - use_duo_boost (bool): Whether to use DUO boost for confidence adjustment. - dlo_boost_coef (float): Coefficient for DLO boost. - s_sim_corr (bool): Whether to use shape similarity correction. - use_rich_s (bool): Whether to use rich shape features. - use_sb (bool): Whether to use soft-BIoU. - use_vt (bool): Whether to use visual tracking. - with_reid (bool): Whether to use re-identification.</p> <p>Attributes: - frame_count (int): Counter for the frames processed. - active_tracks (list): List to hold active tracks. - trackers (List[KalmanBoxTracker]): List of active Kalman filter trackers. - cmc: Camera motion compensation object. - reid_model: Re-identification model instance (if with_reid=True).</p> Source code in <code>boxmot/trackers/boosttrack/boosttrack.py</code> <pre><code>class BoostTrack(BaseTracker):\n    \"\"\"\n    Initialize the BoostTrack tracker with various parameters.\n\n    Parameters:\n    - reid_weights (Path): Path to the re-identification model weights.\n    - device (torch.device): Device to run the model on (e.g., 'cpu', 'cuda').\n    - half (bool): Whether to use half-precision (fp16) for faster inference.\n    - det_thresh (float): Detection threshold for considering detections.\n    - max_age (int): Maximum age (in frames) of a track before it is considered lost.\n    - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5.\n    - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n    - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n    - per_class (bool): Enables class-separated tracking.\n    - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True).\n    - asso_func (str): Algorithm name used for data association between detections and tracks.\n    - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.\n\n    BoostTrack-specific parameters:\n    - use_ecc (bool): Whether to use ECC for camera motion compensation.\n    - min_box_area (int): Minimum box area for detections.\n    - aspect_ratio_thresh (float): Aspect ratio threshold for detections.\n    - cmc_method (str): Method for camera motion compensation.\n    - lambda_iou (float): Weight for IoU-based association.\n    - lambda_mhd (float): Weight for Mahalanobis distance-based association.\n    - lambda_shape (float): Weight for shape-based association.\n    - use_dlo_boost (bool): Whether to use DLO boost for confidence adjustment.\n    - use_duo_boost (bool): Whether to use DUO boost for confidence adjustment.\n    - dlo_boost_coef (float): Coefficient for DLO boost.\n    - s_sim_corr (bool): Whether to use shape similarity correction.\n    - use_rich_s (bool): Whether to use rich shape features.\n    - use_sb (bool): Whether to use soft-BIoU.\n    - use_vt (bool): Whether to use visual tracking.\n    - with_reid (bool): Whether to use re-identification.\n\n    Attributes:\n    - frame_count (int): Counter for the frames processed.\n    - active_tracks (list): List to hold active tracks.\n    - trackers (List[KalmanBoxTracker]): List of active Kalman filter trackers.\n    - cmc: Camera motion compensation object.\n    - reid_model: Re-identification model instance (if with_reid=True).\n    \"\"\"\n\n    def __init__(\n        self,\n        reid_weights,\n        device,\n        half: bool,\n        # BaseTracker parameters \n        det_thresh: float = 0.6,\n        max_age: int = 60,\n        max_obs: int = 50, \n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        per_class: bool = False,\n        nr_classes: int = 80,  \n        asso_func: str = \"iou\",  \n        is_obb: bool = False,\n        # BoostTrack-specific parameters\n        use_ecc: bool = True,\n        min_box_area: int = 10,\n        aspect_ratio_thresh: float = 1.6,\n        cmc_method: str = \"ecc\",\n        lambda_iou: float = 0.5,\n        lambda_mhd: float = 0.25,\n        lambda_shape: float = 0.25,\n        use_dlo_boost: bool = True,\n        use_duo_boost: bool = True,\n        dlo_boost_coef: float = 0.65,\n        s_sim_corr: bool = False,\n        use_rich_s: bool = False,\n        use_sb: bool = False,\n        use_vt: bool = False,\n        with_reid: bool = False,\n        **kwargs  # Additional BaseTracker parameters\n    ):\n\n        # Forward per_class and any additional parameters to BaseTracker\n        super().__init__(\n            det_thresh=det_thresh,\n            max_age=max_age,\n            max_obs=max_obs,\n            min_hits=min_hits,\n            iou_threshold=iou_threshold,\n            per_class=per_class,\n            nr_classes=nr_classes,\n            asso_func=asso_func,\n            is_obb=is_obb,\n            **kwargs\n        )\n        self.active_tracks = []\n        self.frame_count = 0\n        self.trackers: List[KalmanBoxTracker] = []\n\n        # Parameters for BoostTrack (these can be tuned as needed)\n        self.max_age = max_age            # maximum allowed frames without update\n        self.min_hits = min_hits          # minimum hits to output a track\n        self.det_thresh = det_thresh      # detection confidence threshold\n        self.iou_threshold = iou_threshold   # association IoU threshold\n        self.use_ecc = use_ecc            # use ECC for camera motion compensation\n        self.min_box_area = min_box_area  # minimum box area for detections\n        self.aspect_ratio_thresh = aspect_ratio_thresh  # aspect ratio threshold for detections\n        self.cmc_method = cmc_method\n\n        self.lambda_iou = lambda_iou\n        self.lambda_mhd = lambda_mhd\n        self.lambda_shape = lambda_shape\n        self.use_dlo_boost = use_dlo_boost\n        self.use_duo_boost = use_duo_boost\n        self.dlo_boost_coef = dlo_boost_coef\n        self.s_sim_corr = s_sim_corr\n\n        self.use_rich_s = use_rich_s\n        self.use_sb = use_sb\n        self.use_vt = use_vt\n\n        self.with_reid = with_reid\n\n        if self.with_reid:\n            self.reid_model = ReidAutoBackend(weights=reid_weights, device=device, half=half).model\n        else:\n            self.reid_model = None\n\n        if self.use_ecc:\n            self.cmc = get_cmc_method(cmc_method)()\n        else:\n            self.cmc = None\n\n        LOGGER.success(\"Initialized BoostTrack\")\n\n    @BaseTracker.setup_decorator\n    @BaseTracker.per_class_decorator\n    def update(self, dets: np.ndarray, img: np.ndarray, embs: Optional[np.ndarray] = None) -&gt; np.ndarray:\n        \"\"\"\n        Update the tracker with detections and an image.\n\n        Args:\n          dets (np.ndarray): Detection boxes in the format [[x1,y1,x2,y2,score], ...]\n          img (np.ndarray): The current image frame.\n          embs (Optional[np.ndarray]): Optional precomputed embeddings.\n\n        Returns:\n          np.ndarray: Tracked objects in the format\n                      [x1, y1, x2, y2, id, confidence, cls, det_ind]\n                      (with cls and det_ind set to -1 if unused)\n        \"\"\"\n        self.check_inputs(dets=dets, embs=embs, img=img)\n\n        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n\n        self.frame_count += 1\n\n        if self.cmc is not None:\n            transform = self.cmc.apply(img, dets)\n            for trk in self.trackers:\n                trk.camera_update(transform)\n\n        trks = []\n        confs = []\n\n        for trk in self.trackers:\n            pos = trk.predict()[0]\n            conf = trk.get_confidence()\n            confs.append(conf)\n            trks.append(np.concatenate([pos, [conf]]))\n        trks_np = np.vstack(trks) if len(trks) &gt; 0 else np.empty((0, 5))\n\n        if self.use_dlo_boost:\n            dets = self.dlo_confidence_boost(dets)\n        if self.use_duo_boost:\n            dets = self.duo_confidence_boost(dets)\n\n        dets_embs = np.ones((dets.shape[0], 1))\n        if dets.size &gt; 0:\n            remain_inds = dets[:, 4] &gt;= self.det_thresh\n            dets = dets[remain_inds]\n            scores = dets[:, 4]\n\n            if self.with_reid:\n                if embs is not None:\n                    dets_embs = embs[remain_inds]\n                else:\n                    dets_embs = self.reid_model.get_features(dets[:, :4], img)\n        else:\n            scores = np.empty(0)\n            dets_embs = np.ones((dets.shape[0], 1))\n\n        if self.with_reid and len(self.trackers) &gt; 0:\n            tracker_embs = np.array([trk.get_emb() for trk in self.trackers])\n            if dets_embs.shape[0] == 0:\n                emb_cost = np.empty((0, tracker_embs.shape[0]))\n            else:\n                emb_cost = dets_embs.reshape(dets_embs.shape[0], -1) @ tracker_embs.reshape((tracker_embs.shape[0], -1)).T\n        else:\n            emb_cost = None\n\n        mh_dist_matrix = self.get_mh_dist_matrix(dets)\n\n        matched, unmatched_dets, unmatched_trks, _ = associate(\n            dets,\n            trks_np,\n            self.iou_threshold,\n            mahalanobis_distance=mh_dist_matrix,\n            track_confidence=np.array(confs).reshape(-1, 1),\n            detection_confidence=scores,\n            emb_cost=emb_cost,\n            lambda_iou=self.lambda_iou,\n            lambda_mhd=self.lambda_mhd,\n            lambda_shape=self.lambda_shape,\n            s_sim_corr=self.s_sim_corr,\n        )\n\n        if dets.size &gt; 0:\n            trust = (dets[:, 4] - self.det_thresh) / (1 - self.det_thresh)\n            af = 0.95\n            dets_alpha = af + (1 - af) * (1 - trust)\n        else:\n            dets_alpha = np.empty(0)\n\n        for m in matched:\n            self.trackers[m[1]].update(dets[m[0], :])\n            self.trackers[m[1]].update_emb(dets_embs[m[0]], alpha=dets_alpha[m[0]])\n\n        for i in unmatched_dets:\n            if dets[i, 4] &gt;= self.det_thresh:\n                self.trackers.append(\n                    KalmanBoxTracker(dets[i, :], max_obs=self.max_obs, emb=dets_embs[i])\n                )\n\n        outputs = []\n        self.active_tracks = []\n        for trk in self.trackers:\n            d = trk.get_state()[0]\n            if (trk.time_since_update &lt; 1) and (\n                trk.hit_streak &gt;= self.min_hits or self.frame_count &lt;= self.min_hits\n            ):\n                # Format: [x1, y1, x2, y2, id, confidence, cls, det_ind]\n                outputs.append(np.array([d[0], d[1], d[2], d[3], trk.id, trk.conf, trk.cls, trk.det_ind]))\n                self.active_tracks.append(trk)\n\n        self.trackers = [trk for trk in self.trackers if trk.time_since_update &lt;= self.max_age]\n\n        if len(outputs) == 0:\n            return np.empty((0, 8))\n        outputs = np.vstack(outputs)\n        return self.filter_outputs(outputs)\n\n    def filter_outputs(self, outputs: np.ndarray) -&gt; np.ndarray:\n\n        w_arr = outputs[:, 2] - outputs[:, 0]\n        h_arr = outputs[:, 3] - outputs[:, 1]\n\n        vertical_filter = w_arr / h_arr &lt;= self.aspect_ratio_thresh\n        area_filter = w_arr * h_arr &gt; self.min_box_area\n\n        return outputs[vertical_filter &amp; area_filter]\n\n    def get_iou_matrix(self, detections: np.ndarray, buffered: bool = False) -&gt; np.ndarray:\n        trackers = np.zeros((len(self.trackers), 5))\n        for t, trk in enumerate(trackers):\n            pos = self.trackers[t].get_state()[0]\n            trk[:] = [pos[0], pos[1], pos[2], pos[3], self.trackers[t].get_confidence()]\n\n        return iou_batch(detections, trackers) if not buffered else soft_biou_batch(detections, trackers)\n\n    def get_mh_dist_matrix(self, detections: np.ndarray, n_dims: int = 4) -&gt; np.ndarray:\n        if len(self.trackers) == 0:\n            return np.zeros((0, 0))\n        z = np.zeros((len(detections), n_dims), dtype=float)\n        x = np.zeros((len(self.trackers), n_dims), dtype=float)\n        sigma_inv = np.zeros((len(self.trackers), n_dims), dtype=float)\n\n        for i in range(len(detections)):\n            z[i, :n_dims] = convert_bbox_to_z(detections[i, :]).reshape(-1)[:n_dims]\n        for i, trk in enumerate(self.trackers):\n            x[i] = trk.kf.x[:n_dims]\n            sigma_inv[i] = np.reciprocal(np.diag(trk.kf.covariance[:n_dims, :n_dims]))\n        return ((z.reshape((-1, 1, n_dims)) - x.reshape((1, -1, n_dims))) ** 2 *\n                sigma_inv.reshape((1, -1, n_dims))).sum(axis=2)\n\n\n    def duo_confidence_boost(self, detections: np.ndarray) -&gt; np.ndarray:\n        if len(detections) == 0:\n            return detections\n\n        n_dims = 4\n        limit = 13.2767\n        mh_dist = self.get_mh_dist_matrix(detections, n_dims)\n\n        # If there are no existing trackers, bail out immediately\n        if mh_dist.size == 0:\n            return detections\n\n        min_dists = mh_dist.min(1)\n        mask = (min_dists &gt; limit) &amp; (detections[:, 4] &lt; self.det_thresh)\n        boost_inds = np.where(mask)[0]\n        iou_limit = 0.3\n        if len(boost_inds) == 0:\n            return detections\n\n        bdiou = iou_batch(detections[boost_inds], detections[boost_inds]) - np.eye(\n            len(boost_inds)\n        )\n        bdiou_max = bdiou.max(axis=1)\n        remaining = boost_inds[bdiou_max &lt;= iou_limit]\n        args = np.where(bdiou_max &gt; iou_limit)[0]\n        for i in range(len(args)):\n            bi = args[i]\n            tmp = np.where(bdiou[bi] &gt; iou_limit)[0]\n            args_tmp = np.append(\n                np.intersect1d(boost_inds[args], boost_inds[tmp]), boost_inds[bi]\n            )\n            conf_max = np.max(detections[args_tmp, 4])\n            if detections[boost_inds[bi], 4] == conf_max:\n                remaining = np.concatenate([remaining, [boost_inds[bi]]])\n\n        mask_boost = np.zeros_like(detections[:, 4], dtype=bool)\n        mask_boost[remaining] = True\n        detections[:, 4] = np.where(\n            mask_boost, self.det_thresh + 1e-4, detections[:, 4]\n        )\n        return detections\n\n    def dlo_confidence_boost(self, detections: np.ndarray) -&gt; np.ndarray:\n        if len(detections) == 0:\n            return detections\n\n        sbiou_matrix = self.get_iou_matrix(detections, True)\n        if sbiou_matrix.size == 0:\n            return detections\n\n        trackers = np.zeros((len(self.trackers), 6))\n        for t, trk in enumerate(self.trackers):\n            pos = trk.get_state()[0]\n            trackers[t] = [pos[0], pos[1], pos[2], pos[3], 0, trk.time_since_update - 1]\n\n        if self.use_rich_s:\n            mhd_sim = MhDist_similarity(self.get_mh_dist_matrix(detections), 1)\n            shape_sim = shape_similarity(detections, trackers, self.s_sim_corr)\n            S = (mhd_sim + shape_sim + sbiou_matrix) / 3\n        else:\n            S = self.get_iou_matrix(detections, False)\n\n        if not self.use_sb and not self.use_vt:\n            max_s = S.max(1)\n            detections[:, 4] = np.maximum(detections[:, 4], max_s * self.dlo_boost_coef)\n            return detections\n\n        if self.use_sb:\n            max_s = S.max(1)\n            alpha = 0.65\n            detections[:, 4] = np.maximum(\n                detections[:, 4], alpha * detections[:, 4] + (1 - alpha) * max_s**1.5\n            )\n        if self.use_vt:\n            threshold_s = 0.95\n            threshold_e = 0.8\n            n_steps = 20\n            # alpha = (threshold_s - threshold_e) / n_steps # todo alpha is not being used probably a bug\n            tmp = (S &gt; np.maximum(\n                threshold_s - np.array([trk.time_since_update - 1 for trk in self.trackers]),\n                                    threshold_e)).max(1)\n            scores = detections[:, 4].copy()\n            scores[tmp] = np.maximum(scores[tmp], self.det_thresh + 1e-5)\n            detections[:, 4] = scores\n        return detections\n</code></pre>"},{"location":"trackers/boosttrack/#boxmot.trackers.boosttrack.boosttrack.BoostTrack.update","title":"<code>update(dets, img, embs=None)</code>","text":"<p>Update the tracker with detections and an image.</p> <p>Parameters:</p> Name Type Description Default <code>dets</code> <code>ndarray</code> <p>Detection boxes in the format [[x1,y1,x2,y2,score], ...]</p> required <code>img</code> <code>ndarray</code> <p>The current image frame.</p> required <code>embs</code> <code>Optional[ndarray]</code> <p>Optional precomputed embeddings.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Tracked objects in the format           [x1, y1, x2, y2, id, confidence, cls, det_ind]           (with cls and det_ind set to -1 if unused)</p> Source code in <code>boxmot/trackers/boosttrack/boosttrack.py</code> <pre><code>@BaseTracker.setup_decorator\n@BaseTracker.per_class_decorator\ndef update(self, dets: np.ndarray, img: np.ndarray, embs: Optional[np.ndarray] = None) -&gt; np.ndarray:\n    \"\"\"\n    Update the tracker with detections and an image.\n\n    Args:\n      dets (np.ndarray): Detection boxes in the format [[x1,y1,x2,y2,score], ...]\n      img (np.ndarray): The current image frame.\n      embs (Optional[np.ndarray]): Optional precomputed embeddings.\n\n    Returns:\n      np.ndarray: Tracked objects in the format\n                  [x1, y1, x2, y2, id, confidence, cls, det_ind]\n                  (with cls and det_ind set to -1 if unused)\n    \"\"\"\n    self.check_inputs(dets=dets, embs=embs, img=img)\n\n    dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n\n    self.frame_count += 1\n\n    if self.cmc is not None:\n        transform = self.cmc.apply(img, dets)\n        for trk in self.trackers:\n            trk.camera_update(transform)\n\n    trks = []\n    confs = []\n\n    for trk in self.trackers:\n        pos = trk.predict()[0]\n        conf = trk.get_confidence()\n        confs.append(conf)\n        trks.append(np.concatenate([pos, [conf]]))\n    trks_np = np.vstack(trks) if len(trks) &gt; 0 else np.empty((0, 5))\n\n    if self.use_dlo_boost:\n        dets = self.dlo_confidence_boost(dets)\n    if self.use_duo_boost:\n        dets = self.duo_confidence_boost(dets)\n\n    dets_embs = np.ones((dets.shape[0], 1))\n    if dets.size &gt; 0:\n        remain_inds = dets[:, 4] &gt;= self.det_thresh\n        dets = dets[remain_inds]\n        scores = dets[:, 4]\n\n        if self.with_reid:\n            if embs is not None:\n                dets_embs = embs[remain_inds]\n            else:\n                dets_embs = self.reid_model.get_features(dets[:, :4], img)\n    else:\n        scores = np.empty(0)\n        dets_embs = np.ones((dets.shape[0], 1))\n\n    if self.with_reid and len(self.trackers) &gt; 0:\n        tracker_embs = np.array([trk.get_emb() for trk in self.trackers])\n        if dets_embs.shape[0] == 0:\n            emb_cost = np.empty((0, tracker_embs.shape[0]))\n        else:\n            emb_cost = dets_embs.reshape(dets_embs.shape[0], -1) @ tracker_embs.reshape((tracker_embs.shape[0], -1)).T\n    else:\n        emb_cost = None\n\n    mh_dist_matrix = self.get_mh_dist_matrix(dets)\n\n    matched, unmatched_dets, unmatched_trks, _ = associate(\n        dets,\n        trks_np,\n        self.iou_threshold,\n        mahalanobis_distance=mh_dist_matrix,\n        track_confidence=np.array(confs).reshape(-1, 1),\n        detection_confidence=scores,\n        emb_cost=emb_cost,\n        lambda_iou=self.lambda_iou,\n        lambda_mhd=self.lambda_mhd,\n        lambda_shape=self.lambda_shape,\n        s_sim_corr=self.s_sim_corr,\n    )\n\n    if dets.size &gt; 0:\n        trust = (dets[:, 4] - self.det_thresh) / (1 - self.det_thresh)\n        af = 0.95\n        dets_alpha = af + (1 - af) * (1 - trust)\n    else:\n        dets_alpha = np.empty(0)\n\n    for m in matched:\n        self.trackers[m[1]].update(dets[m[0], :])\n        self.trackers[m[1]].update_emb(dets_embs[m[0]], alpha=dets_alpha[m[0]])\n\n    for i in unmatched_dets:\n        if dets[i, 4] &gt;= self.det_thresh:\n            self.trackers.append(\n                KalmanBoxTracker(dets[i, :], max_obs=self.max_obs, emb=dets_embs[i])\n            )\n\n    outputs = []\n    self.active_tracks = []\n    for trk in self.trackers:\n        d = trk.get_state()[0]\n        if (trk.time_since_update &lt; 1) and (\n            trk.hit_streak &gt;= self.min_hits or self.frame_count &lt;= self.min_hits\n        ):\n            # Format: [x1, y1, x2, y2, id, confidence, cls, det_ind]\n            outputs.append(np.array([d[0], d[1], d[2], d[3], trk.id, trk.conf, trk.cls, trk.det_ind]))\n            self.active_tracks.append(trk)\n\n    self.trackers = [trk for trk in self.trackers if trk.time_since_update &lt;= self.max_age]\n\n    if len(outputs) == 0:\n        return np.empty((0, 8))\n    outputs = np.vstack(outputs)\n    return self.filter_outputs(outputs)\n</code></pre>"},{"location":"trackers/botsort/","title":"BotSort","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Initialize the BotSort tracker with various parameters.</p> <p>Parameters: - reid_weights (Path): Path to the re-identification model weights. - device (torch.device): Device to run the model on (e.g., 'cpu', 'cuda'). - half (bool): Whether to use half-precision (fp16) for faster inference. - det_thresh (float): Detection threshold for considering detections. - max_age (int): Maximum age (in frames) of a track before it is considered lost. - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5. - min_hits (int): Minimum number of detection hits before a track is considered confirmed. - iou_threshold (float): IOU threshold for determining match between detection and tracks. - per_class (bool): Enables class-separated tracking. - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True). - asso_func (str): Algorithm name used for data association between detections and tracks. - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.</p> <p>BotSort-specific parameters: - track_high_thresh (float): Detection confidence threshold for first association. - track_low_thresh (float): Detection confidence threshold for ignoring detections. - new_track_thresh (float): Threshold for creating a new track. - track_buffer (int): Frames to keep a track alive after last detection. - match_thresh (float): Matching threshold for data association. - proximity_thresh (float): IoU threshold for first-round association. - appearance_thresh (float): Appearance embedding distance threshold for ReID. - cmc_method (str): Method for correcting camera motion, e.g., \"sof\" (simple optical flow). - frame_rate (int): Video frame rate, used to scale the track buffer. - fuse_first_associate (bool): Fuse appearance and motion in the first association step. - with_reid (bool): Use ReID features for association.</p> <p>Attributes: - frame_count (int): Counter for the frames processed. - active_tracks (list): List to hold active tracks. - lost_stracks (list[STrack]): List of lost tracks. - removed_stracks (list[STrack]): List of removed tracks. - buffer_size (int): Size of the track buffer based on frame rate. - max_time_lost (int): Maximum time a track can be lost. - kalman_filter (KalmanFilterXYWH): Kalman filter for motion prediction.</p> Source code in <code>boxmot/trackers/botsort/botsort.py</code> <pre><code>class BotSort(BaseTracker):\n    \"\"\"\n    Initialize the BotSort tracker with various parameters.\n\n    Parameters:\n    - reid_weights (Path): Path to the re-identification model weights.\n    - device (torch.device): Device to run the model on (e.g., 'cpu', 'cuda').\n    - half (bool): Whether to use half-precision (fp16) for faster inference.\n    - det_thresh (float): Detection threshold for considering detections.\n    - max_age (int): Maximum age (in frames) of a track before it is considered lost.\n    - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5.\n    - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n    - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n    - per_class (bool): Enables class-separated tracking.\n    - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True).\n    - asso_func (str): Algorithm name used for data association between detections and tracks.\n    - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.\n\n    BotSort-specific parameters:\n    - track_high_thresh (float): Detection confidence threshold for first association.\n    - track_low_thresh (float): Detection confidence threshold for ignoring detections.\n    - new_track_thresh (float): Threshold for creating a new track.\n    - track_buffer (int): Frames to keep a track alive after last detection.\n    - match_thresh (float): Matching threshold for data association.\n    - proximity_thresh (float): IoU threshold for first-round association.\n    - appearance_thresh (float): Appearance embedding distance threshold for ReID.\n    - cmc_method (str): Method for correcting camera motion, e.g., \"sof\" (simple optical flow).\n    - frame_rate (int): Video frame rate, used to scale the track buffer.\n    - fuse_first_associate (bool): Fuse appearance and motion in the first association step.\n    - with_reid (bool): Use ReID features for association.\n\n    Attributes:\n    - frame_count (int): Counter for the frames processed.\n    - active_tracks (list): List to hold active tracks.\n    - lost_stracks (list[STrack]): List of lost tracks.\n    - removed_stracks (list[STrack]): List of removed tracks.\n    - buffer_size (int): Size of the track buffer based on frame rate.\n    - max_time_lost (int): Maximum time a track can be lost.\n    - kalman_filter (KalmanFilterXYWH): Kalman filter for motion prediction.\n    \"\"\"\n\n    def __init__(\n        self,\n        reid_weights: Path,\n        device: torch.device,\n        half: bool,\n        # BaseTracker parameters\n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        max_obs: int = 50,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        per_class: bool = False,\n        nr_classes: int = 80,\n        asso_func: str = \"iou\",\n        is_obb: bool = False,\n        # BotSort-specific parameters\n        track_high_thresh: float = 0.5,\n        track_low_thresh: float = 0.1,\n        new_track_thresh: float = 0.6,\n        track_buffer: int = 30,\n        match_thresh: float = 0.8,\n        proximity_thresh: float = 0.5,\n        appearance_thresh: float = 0.25,\n        cmc_method: str = \"ecc\",\n        frame_rate: int = 30,\n        fuse_first_associate: bool = False,\n        with_reid: bool = True,\n        **kwargs  # Additional BaseTracker parameters\n    ):\n        # Forward all BaseTracker parameters explicitly\n        super().__init__(\n            det_thresh=det_thresh,\n            max_age=max_age,\n            max_obs=max_obs,\n            min_hits=min_hits,\n            iou_threshold=iou_threshold,\n            per_class=per_class,\n            nr_classes=nr_classes,\n            asso_func=asso_func,\n            is_obb=is_obb,\n            **kwargs\n        )\n\n        self.lost_stracks = []  # type: list[STrack]\n        self.removed_stracks = []  # type: list[STrack]\n        BaseTrack.clear_count()\n\n        self.per_class = per_class\n        self.track_high_thresh = track_high_thresh\n        self.track_low_thresh = track_low_thresh\n        self.new_track_thresh = new_track_thresh\n        self.match_thresh = match_thresh\n\n        self.buffer_size = int(frame_rate / 30.0 * track_buffer)\n        self.max_time_lost = self.buffer_size\n        self.kalman_filter = KalmanFilterXYWH()\n\n        # ReID module\n        self.proximity_thresh = proximity_thresh\n        self.appearance_thresh = appearance_thresh\n        self.with_reid = with_reid\n        if self.with_reid:\n            self.model = ReidAutoBackend(\n                weights=reid_weights, device=device, half=half\n            ).model\n\n        self.cmc = get_cmc_method(cmc_method)()\n        self.fuse_first_associate = fuse_first_associate\n\n        LOGGER.success(\"Initialized BotSort\")\n\n    @BaseTracker.setup_decorator\n    @BaseTracker.per_class_decorator\n    def update(\n        self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n    ) -&gt; np.ndarray:\n        self.check_inputs(dets, img, embs)\n        self.frame_count += 1\n\n        activated_stracks, refind_stracks, lost_stracks, removed_stracks = [], [], [], []\n\n        # Preprocess detections\n        dets, dets_first, embs_first, dets_second = self._split_detections(dets, embs)\n\n        # Extract appearance features\n        if self.with_reid and embs is None:\n            features_high = self.model.get_features(dets_first[:, 0:4], img)\n        else:\n            features_high = embs_first if embs_first is not None else []\n\n        # Create detections\n        detections = self._create_detections(dets_first, features_high)\n\n        # Separate unconfirmed and active tracks\n        unconfirmed, active_tracks = self._separate_tracks()\n\n        strack_pool = joint_stracks(active_tracks, self.lost_stracks)\n\n        # First association\n        matches_first, u_track_first, u_detection_first = self._first_association(\n            dets,\n            dets_first,\n            active_tracks,\n            unconfirmed,\n            img,\n            detections,\n            activated_stracks,\n            refind_stracks,\n            strack_pool,\n        )\n\n        # Second association\n        matches_second, u_track_second, u_detection_second = self._second_association(\n            dets_second,\n            activated_stracks,\n            lost_stracks,\n            refind_stracks,\n            u_track_first,\n            strack_pool,\n        )\n\n        # Handle unconfirmed tracks\n        matches_unc, u_track_unc, u_detection_unc = self._handle_unconfirmed_tracks(\n            u_detection_first,\n            detections,\n            activated_stracks,\n            removed_stracks,\n            unconfirmed,\n        )\n\n        # Initialize new tracks\n        self._initialize_new_tracks(\n            u_detection_unc,\n            activated_stracks,\n            [detections[i] for i in u_detection_first],\n        )\n\n        # Update lost and removed tracks\n        self._update_track_states(removed_stracks)\n\n        # Merge and prepare output\n        return self._prepare_output(\n            activated_stracks, refind_stracks, lost_stracks, removed_stracks\n        )\n\n    def _split_detections(self, dets, embs):\n        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n        confs = dets[:, 4]\n        second_mask = np.logical_and(\n            confs &gt; self.track_low_thresh, confs &lt; self.track_high_thresh\n        )\n        dets_second = dets[second_mask]\n        first_mask = confs &gt; self.track_high_thresh\n        dets_first = dets[first_mask]\n        embs_first = embs[first_mask] if embs is not None else None\n        return dets, dets_first, embs_first, dets_second\n\n    def _create_detections(self, dets_first, features_high):\n        if len(dets_first) &gt; 0:\n            if self.with_reid:\n                detections = [\n                    STrack(det, f, max_obs=self.max_obs)\n                    for (det, f) in zip(dets_first, features_high)\n                ]\n            else:\n                detections = [STrack(det, max_obs=self.max_obs) for det in dets_first]\n        else:\n            detections = []\n        return detections\n\n    def _separate_tracks(self):\n        unconfirmed, active_tracks = [], []\n        for track in self.active_tracks:\n            if not track.is_activated:\n                unconfirmed.append(track)\n            else:\n                active_tracks.append(track)\n        return unconfirmed, active_tracks\n\n    def _first_association(\n        self,\n        dets,\n        dets_first,\n        active_tracks,\n        unconfirmed,\n        img,\n        detections,\n        activated_stracks,\n        refind_stracks,\n        strack_pool,\n    ):\n\n        STrack.multi_predict(strack_pool)\n\n        # Fix camera motion\n        warp = self.cmc.apply(img, dets)\n        STrack.multi_gmc(strack_pool, warp)\n        STrack.multi_gmc(unconfirmed, warp)\n\n        # Associate with high confidence detection boxes\n        ious_dists = iou_distance(strack_pool, detections)\n        ious_dists_mask = ious_dists &gt; self.proximity_thresh\n        if self.fuse_first_associate:\n            ious_dists = fuse_score(ious_dists, detections)\n\n        if self.with_reid:\n            emb_dists = embedding_distance(strack_pool, detections) / 2.0\n            emb_dists[emb_dists &gt; self.appearance_thresh] = 1.0\n            emb_dists[ious_dists_mask] = 1.0\n            dists = np.minimum(ious_dists, emb_dists)\n        else:\n            dists = ious_dists\n\n        matches, u_track, u_detection = linear_assignment(\n            dists, thresh=self.match_thresh\n        )\n\n        for itracked, idet in matches:\n            track = strack_pool[itracked]\n            det = detections[idet]\n            if track.state == TrackState.Tracked:\n                track.update(detections[idet], self.frame_count)\n                activated_stracks.append(track)\n            else:\n                track.re_activate(det, self.frame_count, new_id=False)\n                refind_stracks.append(track)\n\n        return matches, u_track, u_detection\n\n    def _second_association(\n        self,\n        dets_second,\n        activated_stracks,\n        lost_stracks,\n        refind_stracks,\n        u_track_first,\n        strack_pool,\n    ):\n        if len(dets_second) &gt; 0:\n            detections_second = [\n                STrack(det, max_obs=self.max_obs) for det in dets_second\n            ]\n        else:\n            detections_second = []\n\n        r_tracked_stracks = [\n            strack_pool[i]\n            for i in u_track_first\n            if strack_pool[i].state == TrackState.Tracked\n        ]\n\n        dists = iou_distance(r_tracked_stracks, detections_second)\n        matches, u_track, u_detection = linear_assignment(dists, thresh=0.5)\n\n        for itracked, idet in matches:\n            track = r_tracked_stracks[itracked]\n            det = detections_second[idet]\n            if track.state == TrackState.Tracked:\n                track.update(det, self.frame_count)\n                activated_stracks.append(track)\n            else:\n                track.re_activate(det, self.frame_count, new_id=False)\n                refind_stracks.append(track)\n\n        for it in u_track:\n            track = r_tracked_stracks[it]\n            if not track.state == TrackState.Lost:\n                track.mark_lost()\n                lost_stracks.append(track)\n\n        return matches, u_track, u_detection\n\n    def _handle_unconfirmed_tracks(\n        self, u_detection, detections, activated_stracks, removed_stracks, unconfirmed\n    ):\n        \"\"\"\n        Handle unconfirmed tracks (tracks with only one detection frame).\n\n        Args:\n            u_detection: Unconfirmed detection indices.\n            detections: Current list of detections.\n            activated_stracks: List of newly activated tracks.\n            removed_stracks: List of tracks to remove.\n        \"\"\"\n        # Only use detections that are unconfirmed (filtered by u_detection)\n        detections = [detections[i] for i in u_detection]\n\n        # Calculate IoU distance between unconfirmed tracks and detections\n        ious_dists = iou_distance(unconfirmed, detections)\n\n        # Apply IoU mask to filter out distances that exceed proximity threshold\n        ious_dists_mask = ious_dists &gt; self.proximity_thresh\n        ious_dists = fuse_score(ious_dists, detections)\n\n        # Fuse scores for IoU-based and embedding-based matching (if applicable)\n        if self.with_reid:\n            emb_dists = embedding_distance(unconfirmed, detections) / 2.0\n            emb_dists[emb_dists &gt; self.appearance_thresh] = 1.0\n            emb_dists[ious_dists_mask] = (\n                1.0  # Apply the IoU mask to embedding distances\n            )\n            dists = np.minimum(ious_dists, emb_dists)\n        else:\n            dists = ious_dists\n\n        # Perform data association using linear assignment on the combined distances\n        matches, u_unconfirmed, u_detection = linear_assignment(dists, thresh=0.7)\n\n        # Update matched unconfirmed tracks\n        for itracked, idet in matches:\n            unconfirmed[itracked].update(detections[idet], self.frame_count)\n            activated_stracks.append(unconfirmed[itracked])\n\n        # Mark unmatched unconfirmed tracks as removed\n        for it in u_unconfirmed:\n            track = unconfirmed[it]\n            track.mark_removed()\n            removed_stracks.append(track)\n\n        return matches, u_unconfirmed, u_detection\n\n    def _initialize_new_tracks(self, u_detections, activated_stracks, detections):\n        for inew in u_detections:\n            track = detections[inew]\n            if track.conf &lt; self.new_track_thresh:\n                continue\n\n            track.activate(self.kalman_filter, self.frame_count)\n            activated_stracks.append(track)\n\n    def _update_tracks(\n        self,\n        matches,\n        strack_pool,\n        detections,\n        activated_stracks,\n        refind_stracks,\n        mark_removed=False,\n    ):\n        # Update or reactivate matched tracks\n        for itracked, idet in matches:\n            track = strack_pool[itracked]\n            det = detections[idet]\n            if track.state == TrackState.Tracked:\n                track.update(det, self.frame_count)\n                activated_stracks.append(track)\n            else:\n                track.re_activate(det, self.frame_count, new_id=False)\n                refind_stracks.append(track)\n\n        # Mark only unmatched tracks as removed, if mark_removed flag is True\n        if mark_removed:\n            unmatched_tracks = [\n                strack_pool[i]\n                for i in range(len(strack_pool))\n                if i not in [m[0] for m in matches]\n            ]\n            for track in unmatched_tracks:\n                track.mark_removed()\n\n    def _update_track_states(self, removed_stracks):\n        for track in self.lost_stracks:\n            if self.frame_count - track.end_frame &gt; self.max_time_lost:\n                track.mark_removed()\n                removed_stracks.append(track)\n\n    def _prepare_output(\n        self, activated_stracks, refind_stracks, lost_stracks, removed_stracks\n    ):\n        self.active_tracks = [\n            t for t in self.active_tracks if t.state == TrackState.Tracked\n        ]\n        self.active_tracks = joint_stracks(self.active_tracks, activated_stracks)\n        self.active_tracks = joint_stracks(self.active_tracks, refind_stracks)\n        self.lost_stracks = sub_stracks(self.lost_stracks, self.active_tracks)\n        self.lost_stracks.extend(lost_stracks)\n        self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)\n        self.removed_stracks.extend(removed_stracks)\n        self.active_tracks, self.lost_stracks = remove_duplicate_stracks(\n            self.active_tracks, self.lost_stracks\n        )\n\n        outputs = [\n            [*t.xyxy, t.id, t.conf, t.cls, t.det_ind]\n            for t in self.active_tracks\n            if t.is_activated\n        ]\n\n        return np.asarray(outputs)\n</code></pre>"},{"location":"trackers/bytetrack/","title":"ByteTrack","text":""},{"location":"trackers/bytetrack/#bytetrack","title":"ByteTrack","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Initialize the ByteTrack tracker with various parameters.</p> <p>Parameters: - det_thresh (float): Detection threshold for considering detections. - max_age (int): Maximum age (in frames) of a track before it is considered lost. - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5. - min_hits (int): Minimum number of detection hits before a track is considered confirmed. - iou_threshold (float): IOU threshold for determining match between detection and tracks. - per_class (bool): Enables class-separated tracking. - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True). - asso_func (str): Algorithm name used for data association between detections and tracks. - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.</p> <p>ByteTrack-specific parameters: - min_conf (float): Threshold for detection confidence. Detections below this threshold are discarded. - track_thresh (float): Threshold for detection confidence. Detections above this threshold are considered for tracking in the first association round. - match_thresh (float): Threshold for the matching step in data association. Controls the maximum distance allowed between tracklets and detections for a match. - track_buffer (int): Number of frames to keep a track alive after it was last detected. - frame_rate (int): Frame rate of the video being processed. Used to scale the track buffer size.</p> <p>Attributes: - frame_count (int): Counter for the frames processed. - active_tracks (list): List to hold active tracks. - lost_stracks (list[STrack]): List of lost tracks. - removed_stracks (list[STrack]): List of removed tracks. - buffer_size (int): Size of the track buffer based on frame rate. - max_time_lost (int): Maximum time a track can be lost. - kalman_filter (KalmanFilterXYAH): Kalman filter for motion prediction.</p> Source code in <code>boxmot/trackers/bytetrack/bytetrack.py</code> <pre><code>class ByteTrack(BaseTracker):\n    \"\"\"\n    Initialize the ByteTrack tracker with various parameters.\n\n    Parameters:\n    - det_thresh (float): Detection threshold for considering detections.\n    - max_age (int): Maximum age (in frames) of a track before it is considered lost.\n    - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5.\n    - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n    - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n    - per_class (bool): Enables class-separated tracking.\n    - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True).\n    - asso_func (str): Algorithm name used for data association between detections and tracks.\n    - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.\n\n    ByteTrack-specific parameters:\n    - min_conf (float): Threshold for detection confidence. Detections below this threshold are discarded.\n    - track_thresh (float): Threshold for detection confidence. Detections above this threshold are considered for tracking in the first association round.\n    - match_thresh (float): Threshold for the matching step in data association. Controls the maximum distance allowed between tracklets and detections for a match.\n    - track_buffer (int): Number of frames to keep a track alive after it was last detected.\n    - frame_rate (int): Frame rate of the video being processed. Used to scale the track buffer size.\n\n    Attributes:\n    - frame_count (int): Counter for the frames processed.\n    - active_tracks (list): List to hold active tracks.\n    - lost_stracks (list[STrack]): List of lost tracks.\n    - removed_stracks (list[STrack]): List of removed tracks.\n    - buffer_size (int): Size of the track buffer based on frame rate.\n    - max_time_lost (int): Maximum time a track can be lost.\n    - kalman_filter (KalmanFilterXYAH): Kalman filter for motion prediction.\n    \"\"\"\n\n    def __init__(\n        self,\n        # BaseTracker parameters\n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        max_obs: int = 50,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        per_class: bool = False,\n        nr_classes: int = 80,\n        asso_func: str = \"iou\",\n        is_obb: bool = False,\n        # ByteTrack-specific parameters\n        min_conf: float = 0.1,\n        track_thresh: float = 0.45,\n        match_thresh: float = 0.8,\n        track_buffer: int = 25,\n        frame_rate: int = 30,\n        **kwargs  # Additional BaseTracker parameters\n    ):\n        # Forward all BaseTracker parameters explicitly\n        super().__init__(\n            det_thresh=det_thresh,\n            max_age=max_age,\n            max_obs=max_obs,\n            min_hits=min_hits,\n            iou_threshold=iou_threshold,\n            per_class=per_class,\n            nr_classes=nr_classes,\n            asso_func=asso_func,\n            is_obb=is_obb,\n            **kwargs\n        )\n\n        # Track lifecycle parameters\n        self.frame_id = 0\n        self.track_buffer = track_buffer\n        self.buffer_size = int(frame_rate / 30.0 * track_buffer)\n        self.max_time_lost = self.buffer_size\n\n        # Detection thresholds\n        self.min_conf = min_conf\n        self.track_thresh = track_thresh\n        self.match_thresh = match_thresh\n        self.det_thresh = track_thresh  # Same as track_thresh\n\n        # Motion model\n        self.kalman_filter = KalmanFilterXYAH()\n\n        self.active_tracks = []  # type: list[STrack]\n        self.lost_stracks = []  # type: list[STrack]\n        self.removed_stracks = []  # type: list[STrack]\n\n        LOGGER.success(\"Initialized ByteTrack\")\n\n    @BaseTracker.setup_decorator\n    @BaseTracker.per_class_decorator\n    def update(\n        self, dets: np.ndarray, img: np.ndarray = None, embs: np.ndarray = None\n    ) -&gt; np.ndarray:\n\n        self.check_inputs(dets, img)\n\n        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n        self.frame_count += 1\n        activated_starcks = []\n        refind_stracks = []\n        lost_stracks = []\n        removed_stracks = []\n        confs = dets[:, 4]\n\n        remain_inds = confs &gt; self.track_thresh\n\n        inds_low = confs &gt; self.min_conf\n        inds_high = confs &lt; self.track_thresh\n        inds_second = np.logical_and(inds_low, inds_high)\n\n        dets_second = dets[inds_second]\n        dets = dets[remain_inds]\n\n        if len(dets) &gt; 0:\n            \"\"\"Detections\"\"\"\n            detections = [STrack(det, max_obs=self.max_obs) for det in dets]\n        else:\n            detections = []\n\n        \"\"\" Add newly detected tracklets to tracked_stracks\"\"\"\n        unconfirmed = []\n        tracked_stracks = []  # type: list[STrack]\n        for track in self.active_tracks:\n            if not track.is_activated:\n                unconfirmed.append(track)\n            else:\n                tracked_stracks.append(track)\n\n        \"\"\" Step 2: First association, with high conf detection boxes\"\"\"\n        strack_pool = joint_stracks(tracked_stracks, self.lost_stracks)\n        # Predict the current location with KF\n        STrack.multi_predict(strack_pool)\n        dists = iou_distance(strack_pool, detections)\n        # if not self.args.mot20:\n        dists = fuse_score(dists, detections)\n        matches, u_track, u_detection = linear_assignment(\n            dists, thresh=self.match_thresh\n        )\n\n        for itracked, idet in matches:\n            track = strack_pool[itracked]\n            det = detections[idet]\n            if track.state == TrackState.Tracked:\n                track.update(detections[idet], self.frame_count)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_count, new_id=False)\n                refind_stracks.append(track)\n\n        \"\"\" Step 3: Second association, with low conf detection boxes\"\"\"\n        # association the untrack to the low conf detections\n        if len(dets_second) &gt; 0:\n            \"\"\"Detections\"\"\"\n            detections_second = [\n                STrack(det_second, max_obs=self.max_obs) for det_second in dets_second\n            ]\n        else:\n            detections_second = []\n        r_tracked_stracks = [\n            strack_pool[i]\n            for i in u_track\n            if strack_pool[i].state == TrackState.Tracked\n        ]\n        dists = iou_distance(r_tracked_stracks, detections_second)\n        matches, u_track, u_detection_second = linear_assignment(dists, thresh=0.5)\n        for itracked, idet in matches:\n            track = r_tracked_stracks[itracked]\n            det = detections_second[idet]\n            if track.state == TrackState.Tracked:\n                track.update(det, self.frame_count)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_count, new_id=False)\n                refind_stracks.append(track)\n\n        for it in u_track:\n            track = r_tracked_stracks[it]\n            if not track.state == TrackState.Lost:\n                track.mark_lost()\n                lost_stracks.append(track)\n\n        \"\"\"Deal with unconfirmed tracks, usually tracks with only one beginning frame\"\"\"\n        detections = [detections[i] for i in u_detection]\n        dists = iou_distance(unconfirmed, detections)\n        # if not self.args.mot20:\n        dists = fuse_score(dists, detections)\n        matches, u_unconfirmed, u_detection = linear_assignment(dists, thresh=0.7)\n        for itracked, idet in matches:\n            unconfirmed[itracked].update(detections[idet], self.frame_count)\n            activated_starcks.append(unconfirmed[itracked])\n        for it in u_unconfirmed:\n            track = unconfirmed[it]\n            track.mark_removed()\n            removed_stracks.append(track)\n\n        \"\"\" Step 4: Init new stracks\"\"\"\n        for inew in u_detection:\n            track = detections[inew]\n            if track.conf &lt; self.det_thresh:\n                continue\n            track.activate(self.kalman_filter, self.frame_count)\n            activated_starcks.append(track)\n        \"\"\" Step 5: Update state\"\"\"\n        for track in self.lost_stracks:\n            if self.frame_count - track.end_frame &gt; self.max_time_lost:\n                track.mark_removed()\n                removed_stracks.append(track)\n\n        self.active_tracks = [\n            t for t in self.active_tracks if t.state == TrackState.Tracked\n        ]\n        self.active_tracks = joint_stracks(self.active_tracks, activated_starcks)\n        self.active_tracks = joint_stracks(self.active_tracks, refind_stracks)\n        self.lost_stracks = sub_stracks(self.lost_stracks, self.active_tracks)\n        self.lost_stracks.extend(lost_stracks)\n        self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)\n        self.removed_stracks.extend(removed_stracks)\n        self.active_tracks, self.lost_stracks = remove_duplicate_stracks(\n            self.active_tracks, self.lost_stracks\n        )\n        # get confs of lost tracks\n        output_stracks = [track for track in self.active_tracks if track.is_activated]\n        outputs = []\n        for t in output_stracks:\n            output = []\n            output.extend(t.xyxy)\n            output.append(t.id)\n            output.append(t.conf)\n            output.append(t.cls)\n            output.append(t.det_ind)\n            outputs.append(output)\n        outputs = np.asarray(outputs)\n        return outputs\n</code></pre>"},{"location":"trackers/deepocsort/","title":"DeepOcSort","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Initialize the DeepOcSort tracker with various parameters.</p> <p>Parameters: - reid_weights (Path): Path to the re-identification model weights. - device (torch.device): Device to run the model on (e.g., 'cpu', 'cuda'). - half (bool): Whether to use half-precision (fp16) for faster inference. - det_thresh (float): Detection threshold for considering detections. - max_age (int): Maximum age (in frames) of a track before it is considered lost. - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5. - min_hits (int): Minimum number of detection hits before a track is considered confirmed. - iou_threshold (float): IOU threshold for determining match between detection and tracks. - per_class (bool): Enables class-separated tracking. - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True). - asso_func (str): Algorithm name used for data association between detections and tracks. - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.</p> <p>DeepOcSort-specific parameters: - delta_t (int): Time window size for motion estimation. - inertia (float): Motion model weight, higher values favor motion consistency. - w_association_emb (float): Weight for embedding association in the matching cost. - alpha_fixed_emb (float): Fixed update rate for embeddings. - aw_param (float): Adaptive weight parameter for cost function. - embedding_off (bool): Whether to disable appearance embedding for tracking. - cmc_off (bool): Whether to disable camera motion compensation. - aw_off (bool): Whether to disable adaptive weights for appearance/motion balance. - Q_xy_scaling (float): Scaling factor for process noise in position coordinates. - Q_s_scaling (float): Scaling factor for process noise in scale coordinates.</p> <p>Attributes: - frame_count (int): Counter for the frames processed. - active_tracks (list): List to hold active tracks. - model: ReID model for appearance feature extraction. - cmc: Camera motion compensation object. - kalman_filter: Kalman filter for motion estimation.</p> Source code in <code>boxmot/trackers/deepocsort/deepocsort.py</code> <pre><code>class DeepOcSort(BaseTracker):\n    \"\"\"\n    Initialize the DeepOcSort tracker with various parameters.\n\n    Parameters:\n    - reid_weights (Path): Path to the re-identification model weights.\n    - device (torch.device): Device to run the model on (e.g., 'cpu', 'cuda').\n    - half (bool): Whether to use half-precision (fp16) for faster inference.\n    - det_thresh (float): Detection threshold for considering detections.\n    - max_age (int): Maximum age (in frames) of a track before it is considered lost.\n    - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5.\n    - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n    - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n    - per_class (bool): Enables class-separated tracking.\n    - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True).\n    - asso_func (str): Algorithm name used for data association between detections and tracks.\n    - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.\n\n    DeepOcSort-specific parameters:\n    - delta_t (int): Time window size for motion estimation.\n    - inertia (float): Motion model weight, higher values favor motion consistency.\n    - w_association_emb (float): Weight for embedding association in the matching cost.\n    - alpha_fixed_emb (float): Fixed update rate for embeddings.\n    - aw_param (float): Adaptive weight parameter for cost function.\n    - embedding_off (bool): Whether to disable appearance embedding for tracking.\n    - cmc_off (bool): Whether to disable camera motion compensation.\n    - aw_off (bool): Whether to disable adaptive weights for appearance/motion balance.\n    - Q_xy_scaling (float): Scaling factor for process noise in position coordinates.\n    - Q_s_scaling (float): Scaling factor for process noise in scale coordinates.\n\n    Attributes:\n    - frame_count (int): Counter for the frames processed.\n    - active_tracks (list): List to hold active tracks.\n    - model: ReID model for appearance feature extraction.\n    - cmc: Camera motion compensation object.\n    - kalman_filter: Kalman filter for motion estimation.\n    \"\"\"\n\n    def __init__(\n        self,\n        reid_weights: Path,\n        device: torch.device,\n        half: bool,\n        # BaseTracker parameters\n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        max_obs: int = 50,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        per_class: bool = False,\n        nr_classes: int = 80,\n        asso_func: str = \"iou\",\n        is_obb: bool = False,\n        # DeepOcSort-specific parameters\n        delta_t: int = 3,\n        inertia: float = 0.2,\n        w_association_emb: float = 0.5,\n        alpha_fixed_emb: float = 0.95,\n        aw_param: float = 0.5,\n        embedding_off: bool = False,\n        cmc_off: bool = False,\n        aw_off: bool = False,\n        Q_xy_scaling: float = 0.01,\n        Q_s_scaling: float = 0.0001,\n        **kwargs  # Additional BaseTracker parameters\n    ):\n        # Forward all BaseTracker parameters explicitly\n        super().__init__(\n            det_thresh=det_thresh,\n            max_age=max_age,\n            max_obs=max_obs,\n            min_hits=min_hits,\n            iou_threshold=iou_threshold,\n            per_class=per_class,\n            nr_classes=nr_classes,\n            asso_func=asso_func,\n            is_obb=is_obb,\n            **kwargs\n        )\n\n        \"\"\"\n        Sets key parameters for SORT\n        \"\"\"\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n        self.det_thresh = det_thresh\n        self.delta_t = delta_t\n        self.asso_func = asso_func\n        self.inertia = inertia\n        self.w_association_emb = w_association_emb\n        self.alpha_fixed_emb = alpha_fixed_emb\n        self.aw_param = aw_param\n        self.per_class = per_class\n        self.Q_xy_scaling = Q_xy_scaling\n        self.Q_s_scaling = Q_s_scaling\n        KalmanBoxTracker.count = 1\n\n        self.model = ReidAutoBackend(\n            weights=reid_weights, device=device, half=half\n        ).model\n        # \"similarity transforms using feature point extraction, optical flow, and RANSAC\"\n        self.cmc = get_cmc_method(\"sof\")()\n        self.embedding_off = embedding_off\n        self.cmc_off = cmc_off\n        self.aw_off = aw_off\n\n        LOGGER.success(\"Initialized DeepOcSort\")\n\n    @BaseTracker.setup_decorator\n    @BaseTracker.per_class_decorator\n    def update(\n        self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Params:\n          dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n        Requires: this method must be called once for each frame even with empty detections\n        (use np.empty((0, 5)) for frames without detections).\n        Returns the a similar array, where the last column is the object ID.\n        NOTE: The number of objects returned may differ from the number of detections provided.\n        \"\"\"\n        # dets, s, c = dets.data\n        # print(dets, s, c)\n        self.check_inputs(dets, img, embs)\n\n        self.frame_count += 1\n        self.height, self.width = img.shape[:2]\n\n        scores = dets[:, 4]\n        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n        assert dets.shape[1] == 7\n        remain_inds = scores &gt; self.det_thresh\n        dets = dets[remain_inds]\n\n        # appearance descriptor extraction\n        if self.embedding_off or dets.shape[0] == 0:\n            dets_embs = np.ones((dets.shape[0], 1))\n        elif embs is not None:\n            dets_embs = embs[remain_inds]\n        else:\n            # (Ndets x X) [512, 1024, 2048]\n            dets_embs = self.model.get_features(dets[:, 0:4], img)\n\n        # CMC\n        if not self.cmc_off:\n            transform = self.cmc.apply(img, dets[:, :4])\n            for trk in self.active_tracks:\n                trk.apply_affine_correction(transform)\n\n        trust = (dets[:, 4] - self.det_thresh) / (1 - self.det_thresh)\n        af = self.alpha_fixed_emb\n        # From [self.alpha_fixed_emb, 1], goes to 1 as detector is less confident\n        dets_alpha = af + (1 - af) * (1 - trust)\n\n        # get predicted locations from existing trackers.\n        trks = np.zeros((len(self.active_tracks), 5))\n        trk_embs = []\n        to_del = []\n        ret = []\n        for t, trk in enumerate(trks):\n            pos = self.active_tracks[t].predict()[0]\n            trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n            if np.any(np.isnan(pos)):\n                to_del.append(t)\n            else:\n                trk_embs.append(self.active_tracks[t].get_emb())\n        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n\n        if len(trk_embs) &gt; 0:\n            trk_embs = np.vstack(trk_embs)\n        else:\n            trk_embs = np.array(trk_embs)\n\n        for t in reversed(to_del):\n            self.active_tracks.pop(t)\n\n        velocities = np.array(\n            [trk.velocity if trk.velocity is not None else np.array((0, 0)) for trk in self.active_tracks])\n        last_boxes = np.array([trk.last_observation for trk in self.active_tracks])\n        k_observations = np.array(\n            [k_previous_obs(trk.observations, trk.age, self.delta_t) for trk in self.active_tracks])\n\n        \"\"\"\n            First round of association\n        \"\"\"\n        # (M detections X N tracks, final score)\n        if self.embedding_off or dets.shape[0] == 0 or trk_embs.shape[0] == 0:\n            stage1_emb_cost = None\n        else:\n            stage1_emb_cost = dets_embs @ trk_embs.T\n        matched, unmatched_dets, unmatched_trks = associate(\n            dets[:, 0:5],\n            trks,\n            self.asso_func,\n            self.iou_threshold,\n            velocities,\n            k_observations,\n            self.inertia,\n            img.shape[1],  # w\n            img.shape[0],  # h\n            stage1_emb_cost,\n            self.w_association_emb,\n            self.aw_off,\n            self.aw_param,\n        )\n        for m in matched:\n            self.active_tracks[m[1]].update(dets[m[0], :])\n            self.active_tracks[m[1]].update_emb(dets_embs[m[0]], alpha=dets_alpha[m[0]])\n\n        \"\"\"\n            Second round of associaton by OCR\n        \"\"\"\n        if unmatched_dets.shape[0] &gt; 0 and unmatched_trks.shape[0] &gt; 0:\n            left_dets = dets[unmatched_dets]\n            left_dets_embs = dets_embs[unmatched_dets]\n            left_trks = last_boxes[unmatched_trks]\n            left_trks_embs = trk_embs[unmatched_trks]\n\n            iou_left = self.asso_func(left_dets, left_trks)\n            # TODO: is better without this\n            emb_cost_left = left_dets_embs @ left_trks_embs.T\n            if self.embedding_off:\n                emb_cost_left = np.zeros_like(emb_cost_left)\n            iou_left = np.array(iou_left)\n            if iou_left.max() &gt; self.iou_threshold:\n                \"\"\"\n                NOTE: by using a lower threshold, e.g., self.iou_threshold - 0.1, you may\n                get a higher performance especially on MOT17/MOT20 datasets. But we keep it\n                uniform here for simplicity\n                \"\"\"\n                rematched_indices = linear_assignment(-iou_left)\n                to_remove_det_indices = []\n                to_remove_trk_indices = []\n                for m in rematched_indices:\n                    det_ind, trk_ind = unmatched_dets[m[0]], unmatched_trks[m[1]]\n                    if iou_left[m[0], m[1]] &lt; self.iou_threshold:\n                        continue\n                    self.active_tracks[trk_ind].update(dets[det_ind, :])\n                    self.active_tracks[trk_ind].update_emb(\n                        dets_embs[det_ind], alpha=dets_alpha[det_ind]\n                    )\n                    to_remove_det_indices.append(det_ind)\n                    to_remove_trk_indices.append(trk_ind)\n                unmatched_dets = np.setdiff1d(\n                    unmatched_dets, np.array(to_remove_det_indices)\n                )\n                unmatched_trks = np.setdiff1d(\n                    unmatched_trks, np.array(to_remove_trk_indices)\n                )\n\n        for m in unmatched_trks:\n            self.active_tracks[m].update(None)\n\n        # create and initialise new trackers for unmatched detections\n        for i in unmatched_dets:\n            trk = KalmanBoxTracker(\n                dets[i],\n                delta_t=self.delta_t,\n                emb=dets_embs[i],\n                alpha=dets_alpha[i],\n                Q_xy_scaling=self.Q_xy_scaling,\n                Q_s_scaling=self.Q_s_scaling,\n                max_obs=self.max_obs,\n            )\n            self.active_tracks.append(trk)\n        i = len(self.active_tracks)\n        for trk in reversed(self.active_tracks):\n            if trk.last_observation.sum() &lt; 0:\n                d = trk.get_state()[0]\n            else:\n                \"\"\"\n                this is optional to use the recent observation or the kalman filter prediction,\n                we didn't notice significant difference here\n                \"\"\"\n                d = trk.last_observation[:4]\n            if (trk.time_since_update &lt; 1) and (\n                trk.hit_streak &gt;= self.min_hits or self.frame_count &lt;= self.min_hits\n            ):\n                # +1 as MOT benchmark requires positive\n                ret.append(\n                    np.concatenate(\n                        (d, [trk.id], [trk.conf], [trk.cls], [trk.det_ind])\n                    ).reshape(1, -1)\n                )\n            i -= 1\n            # remove dead tracklet\n            if trk.time_since_update &gt; self.max_age:\n                self.active_tracks.pop(i)\n        if len(ret) &gt; 0:\n            return np.concatenate(ret)\n        return np.array([])\n</code></pre>"},{"location":"trackers/deepocsort/#boxmot.trackers.deepocsort.deepocsort.DeepOcSort.update","title":"<code>update(dets, img, embs=None)</code>","text":"<p>Requires: this method must be called once for each frame even with empty detections (use np.empty((0, 5)) for frames without detections). Returns the a similar array, where the last column is the object ID. NOTE: The number of objects returned may differ from the number of detections provided.</p> Source code in <code>boxmot/trackers/deepocsort/deepocsort.py</code> <pre><code>@BaseTracker.setup_decorator\n@BaseTracker.per_class_decorator\ndef update(\n    self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Params:\n      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n    Requires: this method must be called once for each frame even with empty detections\n    (use np.empty((0, 5)) for frames without detections).\n    Returns the a similar array, where the last column is the object ID.\n    NOTE: The number of objects returned may differ from the number of detections provided.\n    \"\"\"\n    # dets, s, c = dets.data\n    # print(dets, s, c)\n    self.check_inputs(dets, img, embs)\n\n    self.frame_count += 1\n    self.height, self.width = img.shape[:2]\n\n    scores = dets[:, 4]\n    dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n    assert dets.shape[1] == 7\n    remain_inds = scores &gt; self.det_thresh\n    dets = dets[remain_inds]\n\n    # appearance descriptor extraction\n    if self.embedding_off or dets.shape[0] == 0:\n        dets_embs = np.ones((dets.shape[0], 1))\n    elif embs is not None:\n        dets_embs = embs[remain_inds]\n    else:\n        # (Ndets x X) [512, 1024, 2048]\n        dets_embs = self.model.get_features(dets[:, 0:4], img)\n\n    # CMC\n    if not self.cmc_off:\n        transform = self.cmc.apply(img, dets[:, :4])\n        for trk in self.active_tracks:\n            trk.apply_affine_correction(transform)\n\n    trust = (dets[:, 4] - self.det_thresh) / (1 - self.det_thresh)\n    af = self.alpha_fixed_emb\n    # From [self.alpha_fixed_emb, 1], goes to 1 as detector is less confident\n    dets_alpha = af + (1 - af) * (1 - trust)\n\n    # get predicted locations from existing trackers.\n    trks = np.zeros((len(self.active_tracks), 5))\n    trk_embs = []\n    to_del = []\n    ret = []\n    for t, trk in enumerate(trks):\n        pos = self.active_tracks[t].predict()[0]\n        trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n        if np.any(np.isnan(pos)):\n            to_del.append(t)\n        else:\n            trk_embs.append(self.active_tracks[t].get_emb())\n    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n\n    if len(trk_embs) &gt; 0:\n        trk_embs = np.vstack(trk_embs)\n    else:\n        trk_embs = np.array(trk_embs)\n\n    for t in reversed(to_del):\n        self.active_tracks.pop(t)\n\n    velocities = np.array(\n        [trk.velocity if trk.velocity is not None else np.array((0, 0)) for trk in self.active_tracks])\n    last_boxes = np.array([trk.last_observation for trk in self.active_tracks])\n    k_observations = np.array(\n        [k_previous_obs(trk.observations, trk.age, self.delta_t) for trk in self.active_tracks])\n\n    \"\"\"\n        First round of association\n    \"\"\"\n    # (M detections X N tracks, final score)\n    if self.embedding_off or dets.shape[0] == 0 or trk_embs.shape[0] == 0:\n        stage1_emb_cost = None\n    else:\n        stage1_emb_cost = dets_embs @ trk_embs.T\n    matched, unmatched_dets, unmatched_trks = associate(\n        dets[:, 0:5],\n        trks,\n        self.asso_func,\n        self.iou_threshold,\n        velocities,\n        k_observations,\n        self.inertia,\n        img.shape[1],  # w\n        img.shape[0],  # h\n        stage1_emb_cost,\n        self.w_association_emb,\n        self.aw_off,\n        self.aw_param,\n    )\n    for m in matched:\n        self.active_tracks[m[1]].update(dets[m[0], :])\n        self.active_tracks[m[1]].update_emb(dets_embs[m[0]], alpha=dets_alpha[m[0]])\n\n    \"\"\"\n        Second round of associaton by OCR\n    \"\"\"\n    if unmatched_dets.shape[0] &gt; 0 and unmatched_trks.shape[0] &gt; 0:\n        left_dets = dets[unmatched_dets]\n        left_dets_embs = dets_embs[unmatched_dets]\n        left_trks = last_boxes[unmatched_trks]\n        left_trks_embs = trk_embs[unmatched_trks]\n\n        iou_left = self.asso_func(left_dets, left_trks)\n        # TODO: is better without this\n        emb_cost_left = left_dets_embs @ left_trks_embs.T\n        if self.embedding_off:\n            emb_cost_left = np.zeros_like(emb_cost_left)\n        iou_left = np.array(iou_left)\n        if iou_left.max() &gt; self.iou_threshold:\n            \"\"\"\n            NOTE: by using a lower threshold, e.g., self.iou_threshold - 0.1, you may\n            get a higher performance especially on MOT17/MOT20 datasets. But we keep it\n            uniform here for simplicity\n            \"\"\"\n            rematched_indices = linear_assignment(-iou_left)\n            to_remove_det_indices = []\n            to_remove_trk_indices = []\n            for m in rematched_indices:\n                det_ind, trk_ind = unmatched_dets[m[0]], unmatched_trks[m[1]]\n                if iou_left[m[0], m[1]] &lt; self.iou_threshold:\n                    continue\n                self.active_tracks[trk_ind].update(dets[det_ind, :])\n                self.active_tracks[trk_ind].update_emb(\n                    dets_embs[det_ind], alpha=dets_alpha[det_ind]\n                )\n                to_remove_det_indices.append(det_ind)\n                to_remove_trk_indices.append(trk_ind)\n            unmatched_dets = np.setdiff1d(\n                unmatched_dets, np.array(to_remove_det_indices)\n            )\n            unmatched_trks = np.setdiff1d(\n                unmatched_trks, np.array(to_remove_trk_indices)\n            )\n\n    for m in unmatched_trks:\n        self.active_tracks[m].update(None)\n\n    # create and initialise new trackers for unmatched detections\n    for i in unmatched_dets:\n        trk = KalmanBoxTracker(\n            dets[i],\n            delta_t=self.delta_t,\n            emb=dets_embs[i],\n            alpha=dets_alpha[i],\n            Q_xy_scaling=self.Q_xy_scaling,\n            Q_s_scaling=self.Q_s_scaling,\n            max_obs=self.max_obs,\n        )\n        self.active_tracks.append(trk)\n    i = len(self.active_tracks)\n    for trk in reversed(self.active_tracks):\n        if trk.last_observation.sum() &lt; 0:\n            d = trk.get_state()[0]\n        else:\n            \"\"\"\n            this is optional to use the recent observation or the kalman filter prediction,\n            we didn't notice significant difference here\n            \"\"\"\n            d = trk.last_observation[:4]\n        if (trk.time_since_update &lt; 1) and (\n            trk.hit_streak &gt;= self.min_hits or self.frame_count &lt;= self.min_hits\n        ):\n            # +1 as MOT benchmark requires positive\n            ret.append(\n                np.concatenate(\n                    (d, [trk.id], [trk.conf], [trk.cls], [trk.det_ind])\n                ).reshape(1, -1)\n            )\n        i -= 1\n        # remove dead tracklet\n        if trk.time_since_update &gt; self.max_age:\n            self.active_tracks.pop(i)\n    if len(ret) &gt; 0:\n        return np.concatenate(ret)\n    return np.array([])\n</code></pre>"},{"location":"trackers/ocsort/","title":"OcSort","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Initialize the OcSort tracker with various parameters.</p> <p>Parameters: - det_thresh (float): Detection threshold for considering detections. - max_age (int): Maximum age (in frames) of a track before it is considered lost. - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5. - min_hits (int): Minimum number of detection hits before a track is considered confirmed. - iou_threshold (float): IOU threshold for determining match between detection and tracks. - per_class (bool): Enables class-separated tracking. - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True). - asso_func (str): Algorithm name used for data association between detections and tracks. - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.</p> <p>OcSort-specific parameters: - min_conf (float): Minimum confidence threshold for detections to be considered in second-stage association. - delta_t (int): Time window size for velocity estimation in Kalman Filter. - inertia (float): Motion model weight for velocity direction in matching cost. - use_byte (bool): Whether to use BYTE association in the second association step. - Q_xy_scaling (float): Scaling factor for process noise in position coordinates. - Q_s_scaling (float): Scaling factor for process noise in scale coordinates.</p> <p>Attributes: - frame_count (int): Counter for the frames processed. - active_tracks (list): List to hold active tracks. - trackers (list): List of Kalman filter trackers.</p> Source code in <code>boxmot/trackers/ocsort/ocsort.py</code> <pre><code>class OcSort(BaseTracker):\n    \"\"\"\n    Initialize the OcSort tracker with various parameters.\n\n    Parameters:\n    - det_thresh (float): Detection threshold for considering detections.\n    - max_age (int): Maximum age (in frames) of a track before it is considered lost.\n    - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5.\n    - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n    - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n    - per_class (bool): Enables class-separated tracking.\n    - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True).\n    - asso_func (str): Algorithm name used for data association between detections and tracks.\n    - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.\n\n    OcSort-specific parameters:\n    - min_conf (float): Minimum confidence threshold for detections to be considered in second-stage association.\n    - delta_t (int): Time window size for velocity estimation in Kalman Filter.\n    - inertia (float): Motion model weight for velocity direction in matching cost.\n    - use_byte (bool): Whether to use BYTE association in the second association step.\n    - Q_xy_scaling (float): Scaling factor for process noise in position coordinates.\n    - Q_s_scaling (float): Scaling factor for process noise in scale coordinates.\n\n    Attributes:\n    - frame_count (int): Counter for the frames processed.\n    - active_tracks (list): List to hold active tracks.\n    - trackers (list): List of Kalman filter trackers.\n    \"\"\"\n\n    def __init__(\n        self,\n        # BaseTracker parameters\n        det_thresh: float = 0.2,\n        max_age: int = 30,\n        max_obs: int = 50,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        per_class: bool = False,\n        nr_classes: int = 80,\n        asso_func: str = \"iou\",\n        is_obb: bool = False,\n        # OcSort-specific parameters\n        min_conf: float = 0.1,\n        delta_t: int = 3,\n        inertia: float = 0.2,\n        use_byte: bool = False,\n        Q_xy_scaling: float = 0.01,\n        Q_s_scaling: float = 0.0001,\n        **kwargs  # Additional BaseTracker parameters\n    ):\n        # Forward all BaseTracker parameters explicitly\n        super().__init__(\n            det_thresh=det_thresh,\n            max_age=max_age,\n            max_obs=max_obs,\n            min_hits=min_hits,\n            iou_threshold=iou_threshold,\n            per_class=per_class,\n            nr_classes=nr_classes,\n            asso_func=asso_func,\n            is_obb=is_obb,\n            **kwargs\n        )\n\n        # Store OcSort-specific parameters\n        self.min_conf: float = min_conf\n        self.asso_threshold: float = iou_threshold  # Maintain compatibility with existing code\n        self.delta_t: int = delta_t\n        self.inertia: float = inertia\n        self.use_byte: bool = use_byte\n        self.Q_xy_scaling: float = Q_xy_scaling\n        self.Q_s_scaling: float = Q_s_scaling\n        self.frame_count: int = 0\n        KalmanBoxTracker.count = 0\n\n        # Initialize tracker collections\n        self.active_tracks: list = [] \n\n        LOGGER.success(\"Initialized OcSort\")\n\n    @BaseTracker.setup_decorator\n    @BaseTracker.per_class_decorator\n    def update(\n        self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Params:\n          dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n        Requires: this method must be called once for each frame even with empty detections\n        (use np.empty((0, 5)) for frames without detections).\n        Returns the a similar array, where the last column is the object ID.\n        NOTE: The number of objects returned may differ from the number of detections provided.\n        \"\"\"\n\n        self.check_inputs(dets, img)\n\n        self.frame_count += 1\n        h, w = img.shape[0:2]\n\n        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n        confs = dets[:, 4 + self.is_obb]\n\n        inds_low = confs &gt; self.min_conf\n        inds_high = confs &lt; self.det_thresh\n        inds_second = np.logical_and(\n            inds_low, inds_high\n        )  # self.det_thresh &gt; score &gt; 0.1, for second matching\n        dets_second = dets[inds_second]  # detections for second matching\n        remain_inds = confs &gt; self.det_thresh\n        dets = dets[remain_inds]\n\n        # get predicted locations from existing trackers.\n        trks = np.zeros((len(self.active_tracks), 5 + self.is_obb))\n        to_del = []\n        ret = []\n        for t, trk in enumerate(trks):\n            pos = self.active_tracks[t].predict()[0]\n            trk[:] = [pos[i] for i in range(4 + self.is_obb)] + [0]\n            if np.any(np.isnan(pos)):\n                to_del.append(t)\n        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n        for t in reversed(to_del):\n            self.active_tracks.pop(t)\n\n        velocities = np.array(\n            [\n                trk.velocity if trk.velocity is not None else np.array((0, 0))\n                for trk in self.active_tracks\n            ]\n        )\n        last_boxes = np.array([trk.last_observation for trk in self.active_tracks])\n\n        k_observations = np.array(\n            [\n                k_previous_obs(\n                    trk.observations, trk.age, self.delta_t, is_obb=self.is_obb\n                )\n                for trk in self.active_tracks\n            ]\n        )\n\n        \"\"\"\n            First round of association\n        \"\"\"\n        matched, unmatched_dets, unmatched_trks = associate(\n            dets[:, 0 : 5 + self.is_obb],\n            trks,\n            self.asso_func,\n            self.asso_threshold,\n            velocities,\n            k_observations,\n            self.inertia,\n            w,\n            h,\n        )\n        for m in matched:\n            self.active_tracks[m[1]].update(\n                dets[m[0], :-2], dets[m[0], -2], dets[m[0], -1]\n            )\n\n        \"\"\"\n            Second round of associaton by OCR\n        \"\"\"\n        # BYTE association\n        if self.use_byte and len(dets_second) &gt; 0 and unmatched_trks.shape[0] &gt; 0:\n            u_trks = trks[unmatched_trks]\n            iou_left = self.asso_func(\n                dets_second, u_trks\n            )  # iou between low score detections and unmatched tracks\n            iou_left = np.array(iou_left)\n            if iou_left.max() &gt; self.asso_threshold:\n                \"\"\"\n                NOTE: by using a lower threshold, e.g., self.asso_threshold - 0.1, you may\n                get a higher performance especially on MOT17/MOT20 datasets. But we keep it\n                uniform here for simplicity\n                \"\"\"\n                matched_indices = linear_assignment(-iou_left)\n                to_remove_trk_indices = []\n                for m in matched_indices:\n                    det_ind, trk_ind = m[0], unmatched_trks[m[1]]\n                    if iou_left[m[0], m[1]] &lt; self.asso_threshold:\n                        continue\n                    self.active_tracks[trk_ind].update(\n                        dets_second[det_ind, :-2],\n                        dets_second[det_ind, -2],\n                        dets_second[det_ind, -1],\n                    )\n                    to_remove_trk_indices.append(trk_ind)\n                unmatched_trks = np.setdiff1d(\n                    unmatched_trks, np.array(to_remove_trk_indices)\n                )\n\n        if unmatched_dets.shape[0] &gt; 0 and unmatched_trks.shape[0] &gt; 0:\n            left_dets = dets[unmatched_dets]\n            left_trks = last_boxes[unmatched_trks]\n            iou_left = self.asso_func(left_dets, left_trks)\n            iou_left = np.array(iou_left)\n            if iou_left.max() &gt; self.asso_threshold:\n                \"\"\"\n                NOTE: by using a lower threshold, e.g., self.asso_threshold - 0.1, you may\n                get a higher performance especially on MOT17/MOT20 datasets. But we keep it\n                uniform here for simplicity\n                \"\"\"\n                rematched_indices = linear_assignment(-iou_left)\n                to_remove_det_indices = []\n                to_remove_trk_indices = []\n                for m in rematched_indices:\n                    det_ind, trk_ind = unmatched_dets[m[0]], unmatched_trks[m[1]]\n                    if iou_left[m[0], m[1]] &lt; self.asso_threshold:\n                        continue\n                    self.active_tracks[trk_ind].update(\n                        dets[det_ind, :-2], dets[det_ind, -2], dets[det_ind, -1]\n                    )\n                    to_remove_det_indices.append(det_ind)\n                    to_remove_trk_indices.append(trk_ind)\n                unmatched_dets = np.setdiff1d(\n                    unmatched_dets, np.array(to_remove_det_indices)\n                )\n                unmatched_trks = np.setdiff1d(\n                    unmatched_trks, np.array(to_remove_trk_indices)\n                )\n\n        for m in unmatched_trks:\n            self.active_tracks[m].update(None, None, None)\n\n        # create and initialise new trackers for unmatched detections\n        for i in unmatched_dets:\n            if self.is_obb:\n                trk = KalmanBoxTrackerOBB(\n                    dets[i, :-2],\n                    dets[i, -2],\n                    dets[i, -1],\n                    delta_t=self.delta_t,\n                    Q_xy_scaling=self.Q_xy_scaling,\n                    Q_a_scaling=self.Q_s_scaling,\n                    max_obs=self.max_obs,\n                )\n            else:\n                trk = KalmanBoxTracker(\n                    dets[i, :5],\n                    dets[i, 5],\n                    dets[i, 6],\n                    delta_t=self.delta_t,\n                    Q_xy_scaling=self.Q_xy_scaling,\n                    Q_s_scaling=self.Q_s_scaling,\n                    max_obs=self.max_obs,\n                )\n            self.active_tracks.append(trk)\n        i = len(self.active_tracks)\n        for trk in reversed(self.active_tracks):\n            if trk.last_observation.sum() &lt; 0:\n                d = trk.get_state()[0]\n            else:\n                \"\"\"\n                this is optional to use the recent observation or the kalman filter prediction,\n                we didn't notice significant difference here\n                \"\"\"\n                d = trk.last_observation[: 4 + self.is_obb]\n            if (trk.time_since_update &lt; 1) and (\n                trk.hit_streak &gt;= self.min_hits or self.frame_count &lt;= self.min_hits\n            ):\n                # +1 as MOT benchmark requires positive\n                ret.append(\n                    np.concatenate(\n                        (d, [trk.id + 1], [trk.conf], [trk.cls], [trk.det_ind])\n                    ).reshape(1, -1)\n                )\n            i -= 1\n            # remove dead tracklet\n            if trk.time_since_update &gt; self.max_age:\n                self.active_tracks.pop(i)\n        if len(ret) &gt; 0:\n            return np.concatenate(ret)\n        return np.array([])\n</code></pre>"},{"location":"trackers/ocsort/#boxmot.trackers.ocsort.ocsort.OcSort.update","title":"<code>update(dets, img, embs=None)</code>","text":"<p>Requires: this method must be called once for each frame even with empty detections (use np.empty((0, 5)) for frames without detections). Returns the a similar array, where the last column is the object ID. NOTE: The number of objects returned may differ from the number of detections provided.</p> Source code in <code>boxmot/trackers/ocsort/ocsort.py</code> <pre><code>@BaseTracker.setup_decorator\n@BaseTracker.per_class_decorator\ndef update(\n    self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Params:\n      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n    Requires: this method must be called once for each frame even with empty detections\n    (use np.empty((0, 5)) for frames without detections).\n    Returns the a similar array, where the last column is the object ID.\n    NOTE: The number of objects returned may differ from the number of detections provided.\n    \"\"\"\n\n    self.check_inputs(dets, img)\n\n    self.frame_count += 1\n    h, w = img.shape[0:2]\n\n    dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n    confs = dets[:, 4 + self.is_obb]\n\n    inds_low = confs &gt; self.min_conf\n    inds_high = confs &lt; self.det_thresh\n    inds_second = np.logical_and(\n        inds_low, inds_high\n    )  # self.det_thresh &gt; score &gt; 0.1, for second matching\n    dets_second = dets[inds_second]  # detections for second matching\n    remain_inds = confs &gt; self.det_thresh\n    dets = dets[remain_inds]\n\n    # get predicted locations from existing trackers.\n    trks = np.zeros((len(self.active_tracks), 5 + self.is_obb))\n    to_del = []\n    ret = []\n    for t, trk in enumerate(trks):\n        pos = self.active_tracks[t].predict()[0]\n        trk[:] = [pos[i] for i in range(4 + self.is_obb)] + [0]\n        if np.any(np.isnan(pos)):\n            to_del.append(t)\n    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n    for t in reversed(to_del):\n        self.active_tracks.pop(t)\n\n    velocities = np.array(\n        [\n            trk.velocity if trk.velocity is not None else np.array((0, 0))\n            for trk in self.active_tracks\n        ]\n    )\n    last_boxes = np.array([trk.last_observation for trk in self.active_tracks])\n\n    k_observations = np.array(\n        [\n            k_previous_obs(\n                trk.observations, trk.age, self.delta_t, is_obb=self.is_obb\n            )\n            for trk in self.active_tracks\n        ]\n    )\n\n    \"\"\"\n        First round of association\n    \"\"\"\n    matched, unmatched_dets, unmatched_trks = associate(\n        dets[:, 0 : 5 + self.is_obb],\n        trks,\n        self.asso_func,\n        self.asso_threshold,\n        velocities,\n        k_observations,\n        self.inertia,\n        w,\n        h,\n    )\n    for m in matched:\n        self.active_tracks[m[1]].update(\n            dets[m[0], :-2], dets[m[0], -2], dets[m[0], -1]\n        )\n\n    \"\"\"\n        Second round of associaton by OCR\n    \"\"\"\n    # BYTE association\n    if self.use_byte and len(dets_second) &gt; 0 and unmatched_trks.shape[0] &gt; 0:\n        u_trks = trks[unmatched_trks]\n        iou_left = self.asso_func(\n            dets_second, u_trks\n        )  # iou between low score detections and unmatched tracks\n        iou_left = np.array(iou_left)\n        if iou_left.max() &gt; self.asso_threshold:\n            \"\"\"\n            NOTE: by using a lower threshold, e.g., self.asso_threshold - 0.1, you may\n            get a higher performance especially on MOT17/MOT20 datasets. But we keep it\n            uniform here for simplicity\n            \"\"\"\n            matched_indices = linear_assignment(-iou_left)\n            to_remove_trk_indices = []\n            for m in matched_indices:\n                det_ind, trk_ind = m[0], unmatched_trks[m[1]]\n                if iou_left[m[0], m[1]] &lt; self.asso_threshold:\n                    continue\n                self.active_tracks[trk_ind].update(\n                    dets_second[det_ind, :-2],\n                    dets_second[det_ind, -2],\n                    dets_second[det_ind, -1],\n                )\n                to_remove_trk_indices.append(trk_ind)\n            unmatched_trks = np.setdiff1d(\n                unmatched_trks, np.array(to_remove_trk_indices)\n            )\n\n    if unmatched_dets.shape[0] &gt; 0 and unmatched_trks.shape[0] &gt; 0:\n        left_dets = dets[unmatched_dets]\n        left_trks = last_boxes[unmatched_trks]\n        iou_left = self.asso_func(left_dets, left_trks)\n        iou_left = np.array(iou_left)\n        if iou_left.max() &gt; self.asso_threshold:\n            \"\"\"\n            NOTE: by using a lower threshold, e.g., self.asso_threshold - 0.1, you may\n            get a higher performance especially on MOT17/MOT20 datasets. But we keep it\n            uniform here for simplicity\n            \"\"\"\n            rematched_indices = linear_assignment(-iou_left)\n            to_remove_det_indices = []\n            to_remove_trk_indices = []\n            for m in rematched_indices:\n                det_ind, trk_ind = unmatched_dets[m[0]], unmatched_trks[m[1]]\n                if iou_left[m[0], m[1]] &lt; self.asso_threshold:\n                    continue\n                self.active_tracks[trk_ind].update(\n                    dets[det_ind, :-2], dets[det_ind, -2], dets[det_ind, -1]\n                )\n                to_remove_det_indices.append(det_ind)\n                to_remove_trk_indices.append(trk_ind)\n            unmatched_dets = np.setdiff1d(\n                unmatched_dets, np.array(to_remove_det_indices)\n            )\n            unmatched_trks = np.setdiff1d(\n                unmatched_trks, np.array(to_remove_trk_indices)\n            )\n\n    for m in unmatched_trks:\n        self.active_tracks[m].update(None, None, None)\n\n    # create and initialise new trackers for unmatched detections\n    for i in unmatched_dets:\n        if self.is_obb:\n            trk = KalmanBoxTrackerOBB(\n                dets[i, :-2],\n                dets[i, -2],\n                dets[i, -1],\n                delta_t=self.delta_t,\n                Q_xy_scaling=self.Q_xy_scaling,\n                Q_a_scaling=self.Q_s_scaling,\n                max_obs=self.max_obs,\n            )\n        else:\n            trk = KalmanBoxTracker(\n                dets[i, :5],\n                dets[i, 5],\n                dets[i, 6],\n                delta_t=self.delta_t,\n                Q_xy_scaling=self.Q_xy_scaling,\n                Q_s_scaling=self.Q_s_scaling,\n                max_obs=self.max_obs,\n            )\n        self.active_tracks.append(trk)\n    i = len(self.active_tracks)\n    for trk in reversed(self.active_tracks):\n        if trk.last_observation.sum() &lt; 0:\n            d = trk.get_state()[0]\n        else:\n            \"\"\"\n            this is optional to use the recent observation or the kalman filter prediction,\n            we didn't notice significant difference here\n            \"\"\"\n            d = trk.last_observation[: 4 + self.is_obb]\n        if (trk.time_since_update &lt; 1) and (\n            trk.hit_streak &gt;= self.min_hits or self.frame_count &lt;= self.min_hits\n        ):\n            # +1 as MOT benchmark requires positive\n            ret.append(\n                np.concatenate(\n                    (d, [trk.id + 1], [trk.conf], [trk.cls], [trk.det_ind])\n                ).reshape(1, -1)\n            )\n        i -= 1\n        # remove dead tracklet\n        if trk.time_since_update &gt; self.max_age:\n            self.active_tracks.pop(i)\n    if len(ret) &gt; 0:\n        return np.concatenate(ret)\n    return np.array([])\n</code></pre>"},{"location":"trackers/strongsort/","title":"StrongSort","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Initialize the StrongSort tracker with various parameters.</p> <p>Parameters: - reid_weights (Path): Path to the re-identification model weights. - device (torch.device): Device to run the model on (e.g., 'cpu', 'cuda'). - half (bool): Whether to use half-precision (fp16) for faster inference. - det_thresh (float): Detection threshold for considering detections. - max_age (int): Maximum age (in frames) of a track before it is considered lost. - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5. - min_hits (int): Minimum number of detection hits before a track is considered confirmed. - iou_threshold (float): IOU threshold for determining match between detection and tracks. - per_class (bool): Enables class-separated tracking. - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True). - asso_func (str): Algorithm name used for data association between detections and tracks. - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.</p> <p>StrongSort-specific parameters: - min_conf (float): Minimum confidence threshold for detections. - max_cos_dist (float): Maximum cosine distance for ReID feature matching in Nearest Neighbor Distance Metric. - max_iou_dist (float): Maximum IoU distance for data association. - n_init (int): Number of consecutive frames required to confirm a track. - nn_budget (int): Maximum size of the feature library for Nearest Neighbor Distance Metric. - mc_lambda (float): Weight for motion consistency in the track state estimation. - ema_alpha (float): Alpha value for exponential moving average (EMA) update of appearance features.</p> <p>Attributes: - model: ReID model for appearance feature extraction. - tracker: StrongSort tracker instance. - cmc: Camera motion compensation object.</p> Source code in <code>boxmot/trackers/strongsort/strongsort.py</code> <pre><code>class StrongSort(BaseTracker):\n    \"\"\"\n    Initialize the StrongSort tracker with various parameters.\n\n    Parameters:\n    - reid_weights (Path): Path to the re-identification model weights.\n    - device (torch.device): Device to run the model on (e.g., 'cpu', 'cuda').\n    - half (bool): Whether to use half-precision (fp16) for faster inference.\n    - det_thresh (float): Detection threshold for considering detections.\n    - max_age (int): Maximum age (in frames) of a track before it is considered lost.\n    - max_obs (int): Maximum number of historical observations stored for each track. Always greater than max_age by minimum 5.\n    - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n    - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n    - per_class (bool): Enables class-separated tracking.\n    - nr_classes (int): Total number of object classes that the tracker will handle (for per_class=True).\n    - asso_func (str): Algorithm name used for data association between detections and tracks.\n    - is_obb (bool): Work with Oriented Bounding Boxes (OBB) instead of standard axis-aligned bounding boxes.\n\n    StrongSort-specific parameters:\n    - min_conf (float): Minimum confidence threshold for detections.\n    - max_cos_dist (float): Maximum cosine distance for ReID feature matching in Nearest Neighbor Distance Metric.\n    - max_iou_dist (float): Maximum IoU distance for data association.\n    - n_init (int): Number of consecutive frames required to confirm a track.\n    - nn_budget (int): Maximum size of the feature library for Nearest Neighbor Distance Metric.\n    - mc_lambda (float): Weight for motion consistency in the track state estimation.\n    - ema_alpha (float): Alpha value for exponential moving average (EMA) update of appearance features.\n\n    Attributes:\n    - model: ReID model for appearance feature extraction.\n    - tracker: StrongSort tracker instance.\n    - cmc: Camera motion compensation object.\n    \"\"\"\n\n    def __init__(\n        self,\n        reid_weights: Path,\n        device: device,\n        half: bool,\n        # BaseTracker parameters\n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        max_obs: int = 50,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        per_class: bool = False,\n        nr_classes: int = 80,\n        asso_func: str = \"iou\",\n        is_obb: bool = False,\n        # StrongSort-specific parameters\n        min_conf: float = 0.1,\n        max_cos_dist: float = 0.2,\n        max_iou_dist: float = 0.7,\n        n_init: int = 3,\n        nn_budget: int = 100,\n        mc_lambda: float = 0.98,\n        ema_alpha: float = 0.9,\n        **kwargs  # Additional BaseTracker parameters\n    ):\n        # Forward all BaseTracker parameters explicitly\n        super().__init__(\n            det_thresh=det_thresh,\n            max_age=max_age,\n            max_obs=max_obs,\n            min_hits=min_hits,\n            iou_threshold=iou_threshold,\n            per_class=per_class,\n            nr_classes=nr_classes,\n            asso_func=asso_func,\n            is_obb=is_obb,\n            **kwargs\n        )\n\n        # Store StrongSort-specific parameters\n        self.min_conf = min_conf\n\n        # Initialize ReID model\n        self.model = ReidAutoBackend(\n            weights=reid_weights, device=device, half=half\n        ).model\n\n        # Initialize StrongSort tracker\n        self.tracker = Tracker(\n            metric=NearestNeighborDistanceMetric(\"cosine\", max_cos_dist, nn_budget),\n            max_iou_dist=max_iou_dist,\n            max_age=max_age,\n            n_init=n_init,\n            mc_lambda=mc_lambda,\n            ema_alpha=ema_alpha,\n        )\n\n        # Initialize camera motion compensation\n        self.cmc = get_cmc_method(\"ecc\")()\n\n        LOGGER.success(\"Initialized StrongSort\")\n\n    @BaseTracker.per_class_decorator\n    def update(\n        self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n    ) -&gt; np.ndarray:\n        assert isinstance(\n            dets, np.ndarray\n        ), f\"Unsupported 'dets' input format '{type(dets)}', valid format is np.ndarray\"\n        assert isinstance(\n            img, np.ndarray\n        ), f\"Unsupported 'img' input format '{type(img)}', valid format is np.ndarray\"\n        assert (\n            len(dets.shape) == 2\n        ), \"Unsupported 'dets' dimensions, valid number of dimensions is two\"\n        assert (\n            dets.shape[1] == 6\n        ), \"Unsupported 'dets' 2nd dimension lenght, valid lenghts is 6\"\n        if embs is not None:\n            assert (\n                dets.shape[0] == embs.shape[0]\n            ), \"Missmatch between detections and embeddings sizes\"\n\n        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n        remain_inds = dets[:, 4] &gt;= self.min_conf\n        dets = dets[remain_inds]\n\n        xyxy = dets[:, 0:4]\n        confs = dets[:, 4]\n        clss = dets[:, 5]\n        det_ind = dets[:, 6]\n\n        if len(self.tracker.tracks) &gt;= 1:\n            warp_matrix = self.cmc.apply(img, xyxy)\n            for track in self.tracker.tracks:\n                track.camera_update(warp_matrix)\n\n        # extract appearance information for each detection\n        if embs is not None:\n            features = embs[remain_inds]\n        else:\n            features = self.model.get_features(xyxy, img)\n\n        tlwh = xyxy2tlwh(xyxy)\n        detections = [\n            Detection(box, conf, cls, det_ind, feat)\n            for box, conf, cls, det_ind, feat in zip(\n                tlwh, confs, clss, det_ind, features\n            )\n        ]\n\n        # update tracker\n        self.tracker.predict()\n        self.tracker.update(detections)\n\n        # output bbox identities\n        outputs = []\n        for track in self.tracker.tracks:\n            if not track.is_confirmed() or track.time_since_update &gt;= 1:\n                continue\n\n            x1, y1, x2, y2 = track.to_tlbr()\n\n            id = track.id\n            conf = track.conf\n            cls = track.cls\n            det_ind = track.det_ind\n\n            outputs.append(\n                np.concatenate(\n                    ([x1, y1, x2, y2], [id], [conf], [cls], [det_ind])\n                ).reshape(1, -1)\n            )\n        if len(outputs) &gt; 0:\n            return np.concatenate(outputs)\n        return np.array([])\n\n    def reset(self):\n        pass\n</code></pre>"}]}